# 操作系统学习

# 第一章：硬件结构

## ·计算机执行加法的过程：

·以图灵机为例：

- 读写头上有一些部件，比如存储单元、控制单元以及运算单元： 1、存储单元用于存放数据； 2、控制单元用于**识别字符**是数据还是指令，以及控制程序的流程等； 3、运算单元用于执行运算指令；

![image-20230327114730726](D:\typora笔记\操作系统笔记图片\image-20230327114730726.png)	

·读写头先将1，2读取到存储单元中：

![image-20230327115039042](D:\typora笔记\操作系统笔记图片\image-20230327115039042.png)	

·读取到“+”号，控制单元确认该字符不是数字，交给运算单元处理，运算单元进行 加和 操作，并将结果重新存储到状态中；

·计算完成后，运算单元将结果告诉控制单元，控制单元再操作读写头，将结果3写入纸带；

![image-20230327115318269](D:\typora笔记\操作系统笔记图片\image-20230327115318269.png)	

![image-20230327115456055](D:\typora笔记\操作系统笔记图片\image-20230327115456055.png)	



## ·冯诺依曼模型：

·五个结构：**运算器、控制器、存储器、输入设备、输出设备**，这 5 个部分也被称为**冯诺依曼模型**。

·![image-20230327115819855](D:\typora笔记\操作系统笔记图片\image-20230327115819855.png)

运算器、控制器是在中央处理器里的，存储器就我们常见的内存，输入输出设备则是计算机外接的设备，比如键盘就是输入设备，显示器就是输出设备。



### ·总线：

·存储单元与输入输出设备交互，需要依赖总线传输：

·三类总线：数据总线，地址总线，控制总线；

![image-20230327115946019](D:\typora笔记\操作系统笔记图片\image-20230327115946019.png)	

### ·内存：

·以字节为单位存储，一个字节=8位（bit），地址从0开始，最后一个地址位内存总字节数-1；类似数组，**内存读取任何一个数据的速度都一样；**



### ·中央处理器：

·分为32，64位cpu，取决于一次计算多少字节的数据；

- 32 位 CPU 一次可以计算 4 个字节；
- 64 位 CPU 一次可以计算 8 个字节；

**·CPU位宽：**

代表的是 CPU 一次可以计算（运算）的数据量。

·八位CPU一次可以计算0~255范围的数值；这样就无法一次完成计算 `10000 * 500` ，于是为了能**一次计算大数的运算**，CPU 需要支持**多个 byte 一起计算**，所以 CPU **位宽越大**，可以**计算的数值就越大**，比如说 32 位 CPU 能计算的最大整数是 `4294967295`。

**·寄存器、控制单元和逻辑运算单元**

·控制单元负责控制 CPU 工作，逻辑运算单元负责计算，而寄存器可以分为多种类，每种寄存器的功能又不尽相同。



**·寄存器：**

·CPU 中的寄存器主要作用是**存储计算时的数据**。

**为什么需要寄存器？**

因为内存离 CPU 太远了，而寄存器就在 CPU 里，还紧挨着控制单元和逻辑运算单元，自然计算时速度会很快。

（内存与cpu通过总线相连，传输数据需要时间，而寄存器就在cpu内部，相比内存能够更快响应cpu的计算）



**·常见寄存器种类：**

- *通用寄存器*，用来存放**需要进行运算的数据**，比如需要进行加和运算的两个数据。
- *程序计数器*，用来存储 CPU 要执行**下一条指令「所在的内存地址」**，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令「的地址」。
- *指令寄存器*，用来存放当前**正在执行的指令**，也就是指令本身，指令被执行完成之前，指令都存储在这里。



### ·总线：

总线是用于 CPU 和内存以及其他设备之间的通信，分为三种：

- *地址总线*，用于指定 **CPU** 将要操作的**内存地址**；
- *数据总线*，用于读写**内存的数据**；
- *控制总线*，用于**发送和接收信号**，比如中断、设备复位等信号，CPU 收到信号后自然进行响应，这时也需要控制总线；



**·CPU读取内存数据的过程：**

- 首先要通过「地址总线」来指定内存的地址；
- 然后通过「控制总线」控制是读或写命令；
- 最后通过「数据总线」来传输数据；



### ·输入输出设备：

·输入设备向计算机输入数据，计算机经过计算后，把数据输出给输出设备。期间，如果输入设备是键盘，按下按键时是需要和 CPU 进行交互的，这时就需要用到**控制总线**了。









## 线路位宽与 CPU 位宽：

### ·数据通过线路传输原理：

·通过操作电压，低电压0，高电压1；

·在传递101 bin = 5 oct，一条线路需要传输三次电压；为了增加传输效率，可通过增加线路实现并行传输；

·为了避免低效率的串行传输的方式，线路的位宽最好一次就能访问到所有的内存地址。

**线路位宽的意义：**

~~~txt
CPU 想要操作「内存地址」就需要「地址总线」：
·如果地址总线只有 1 条，那每次只能表示 「0 或 1」这两种地址，所以 CPU 能操作的内存地址最大数量为 2（2^1）个（注意，不要理解成同时能操作 2 个内存地址）；
·如果地址总线有 2 条，那么能表示 00、01、10、11 这四种地址，所以 CPU 能操作的内存地址最大数量为 4（2^2）个。

·CPU 操作 4G 大的内存，那么就需要 32 条地址总线，因为 2 ^ 32 = 4G。
~~~

**·CPU位宽与线路位宽的关系：**

·**CPU 的位宽最好不要小于线路位宽**，比如 32 位 CPU 控制 40 位宽的地址总线和数据总线的话，工作起来就会非常复杂且麻烦，所以 32 位的 CPU 最好和 32 位宽的线路搭配，因为 32 位 CPU 一次最多只能操作 32 位宽的地址总线和数据总线。

**·数据的高位，低位：**

~~~txt
如果用 32 位 CPU 去加和两个 64 位大小的数字，就需要把这 2 个 64 位的数字分成 2 个低位 32 位数字和 2 个高位 32 位数字来计算，先加个两个低位的 32 位数字，算出进位，然后加和两个高位的 32 位数字，【最后再加上进位】，就能算出结果了，可以发现 32 位 CPU 并不能一次性计算出加和两个 64 位数字的结果。

对于 64 位 CPU 就可以一次性算出加和两个 64 位数字的结果，因为 64 位 CPU 可以一次读入 64 位的数字，并且 64 位 CPU 内部的逻辑运算单元也支持 64 位数字的计算。
~~~

**·32位与64位的CPU区别：**

但是并不代表 64 位 CPU 性能比 32 位 CPU 高很多，很少应用需要算超过 32 位的数字，所以**如果计算的数额不超过 32 位数字的情况下，32 位和 64 位 CPU 之间没什么区别的，只有当计算超过 32 位数字的情况下，64 位的优势才能体现出来**。

（在32位的计算场景下没有区别，只有64位的计算下才会有区别）



**·CPU与内存大小的关系：**

~~~txt
32 位 CPU 最大只能操作 4GB 内存，就算你装了 8 GB 内存条，也没用。而 64 位 CPU 寻址范围则很大，理论最大的寻址空间为 2^64。
~~~



## ·程序执行的基本过程：

程序实际上是一条一条指令，所以程序的运行过程就是把每一条指令一步一步的执行起来，负责执行指令的就是 CPU 了。

![image-20230327213920769](D:\typora笔记\操作系统笔记图片\image-20230327213920769.png)	

·第一步：

~~~txt
第一步，CPU 读取「程序计数器」的值，这个值是指令的内存地址，然后 CPU 的「控制单元」操作「地址总线」指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过「数据总线」将指令数据传给 CPU，CPU 收到内存传来的数据后，将这个指令数据存入到「指令寄存器」。

小结：CPU先读取【程序计数器】中的指令内存地址，【控制单元】操作【地址总线】访问内存地址，通知内存准备好数据，数据准备好通过【数据总线】将指令传输给CPU，CPU将指令存入【指令寄存器】；
~~~

·第二步：

~~~txt
第二步，「程序计数器」的值自增，表示指向下一条指令。
【这个自增的大小】，由【CPU的位宽决定】，比如 32 位的 CPU，指令是4个字节，需要4个内存地址存放，因此「程序计数器」的值会自增 4；
~~~

·第三步：

~~~txt
第三步，CPU 分析「指令寄存器」中的指令，确定指令的类型和参数，如果是计算类型的指令，就把指令交给「逻辑运算单元」运算；如果是存储类型的指令，则交由「控制单元」执行；
~~~

·小结**（CPU指令周期）**：

一个程序执行的时候，CPU 会根据程序计数器里的内存地址，从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。

CPU 从程序计数器读取指令、到执行、再到下一条指令，这个过程会不断循环，直到**程序执行结束**，这个不断循环的过程被称为 **CPU 的指令周期**。





## ·a=1+2的执行过程：

·a=1+2需要先转为汇编代码，再使用汇编器翻译为机器码--01的指令，此时CPU才能理解该程序；

·第一步：

~~~txt
程序编译过程中，编译器通过分析代码，发现 1 和 2 是数据，于是程序运行时，内存会有个专门的区域来存放这些数据，这个区域就是「数据段」。如下图，数据 1 和 2 的区域位置：

数据 1 被存放到 0x200 位置；
数据 2 被存放到 0x204 位置；
~~~

![image-20230327215543326](D:\typora笔记\操作系统笔记图片\image-20230327215543326.png)	

·编译器会把 `a = 1 + 2` 翻译成 4 条指令，存放到正文段中。如图，这 4 条指令被存放到了 0x100 ~ 0x10c 的区域中：

~~~txt
·0x100 的内容是 load 指令将 0x200 地址中的数据 1 装入到寄存器 R0；
·0x104 的内容是 load 指令将 0x204 地址中的数据 2 装入到寄存器 R1；
·0x108 的内容是 add 指令将寄存器 R0 和 R1 的数据相加，并把结果存放到寄存器 R2；
·0x10c 的内容是 store 指令将寄存器 R2 中的数据存回数据段中的 0x208 地址中，这个地址也就是变量 a 内存中的地址；
~~~

·编译完成后，具体执行程序的时候，程序计数器会被设置为 0x100 地址，然后依次执行这 4 条指令。



## ·指令：

~~~txt
指令的内容是一串二进制数字的机器码，每条指令都有对应的机器码，CPU 通过解析机器码来知道指令的内容。
~~~

不同的 CPU 有不同的指令集，也就是对应着不同的**汇编语言**和不同的**机器码**。

### ·以MIPS指集为例：

~~~txt
MIPS 的指令是一个 32 位的整数，高 6 位代表着操作码，表示这条指令是一条什么样的指令，剩下的 26 位不同指令类型所表示的内容也就不相同，主要有三种类型R、I 和 J。
~~~

![image-20230327224107234](D:\typora笔记\操作系统笔记图片\image-20230327224107234.png)	

- *R 指令*，用在算术和逻辑操作，里面有读取和写入数据的寄存器地址。如果是逻辑位移操作，后面还有位移操作的**「位移量」**，而最后的**「功能码」**则是再前面的操作码不够的时候，**扩展操作码**来表示对应的具体指令的；
- *I 指令*，用在数据传输、条件分支等。这个类型的指令，就没有了位移量和功能码，也没有了第三个寄存器，而是把这三部分直接合并成了一个**地址值或一个常数**；
- *J 指令*，用在跳转，高 6 位之外的 26 位都是一个跳转后的地址；

~~~
「add 指令将寄存器 R0 和 R1 的数据相加，并把结果放入到 R2」，翻译成机器码。
~~~

![image-20230327224634509](D:\typora笔记\操作系统笔记图片\image-20230327224634509.png)	

加和运算 add 指令是属于 R 指令类型：

- add 对应的 MIPS 指令里操作码是 `000000`，以及最末尾的功能码是 `100000`，这些**数值都是固定**的，查一下 **MIPS 指令集的手册**就能知道的；
- rs 代表第一个寄存器 R0 的编号，即 `00000`；
- rt 代表第二个寄存器 R1 的编号，即 `00001`；
- rd 代表目标的临时寄存器 R2 的编号，即 `00010`；
- 因为不是位移操作，所以位移量是 `00000`

把上面这些数字拼在一起就是一条 32 位的 MIPS 加法指令了，那么用 16 进制表示的机器码则是 `0x00011020`。



### ·编码、解码：

~~~
编译器在编译程序的时候，会构造指令，这个过程叫做指令的编码。CPU 执行程序的时候，就会解析指令，这个过程叫作指令的解码。
~~~



### ·指令执行流水线：

~~~
现代大多数 CPU 都使用来流水线的方式来执行指令，所谓的流水线就是把一个任务拆分成多个小任务，于是一条指令通常分为 4 个阶段，称为 4 级流水线，如下图：
~~~

![image-20230327225020485](D:\typora笔记\操作系统笔记图片\image-20230327225020485.png)	

·四个阶段的意义：

（1）fetch：表示CPU从内存中**获取指令**；

（2）decode：CPU对指令进行**解码**操作；

（3）CPU 执行指令，这个部分称为 **Execution（执行指令）**；

（4）CPU 将计算结果存回寄存器或者将寄存器的值存入内存，这个部分称为 **Store（数据回写）**；

**不同阶段由计算机不同部件完成；**

​	![image-20230327225320622](D:\typora笔记\操作系统笔记图片\image-20230327225320622.png)	

- 取指令的阶段，我们的指令是存放在**存储器**里的，实际上，通过【程序计数器】和【指令寄存器】取出指令的过程，是由**控制器**操作的；
- 指令的译码过程，也是由**控制器**进行的；
- 指令执行的过程，无论是进行算术操作、逻辑操作，还是进行数据传输、条件分支操作，都是由**算术逻辑单元**操作的，也就是由**运算器**处理的。但是如果是一个**简单的无条件地址跳转**，则是直接在**控制器**里面完成的，不需要用到运算器。



## ·指令的类型：

·分为5类：

- *数据传输类型的指令*，比如 `store/load` 是寄存器与内存间数据传输的指令，`mov` 是将一个内存地址的数据移动到另一个内存地址的指令；
- *运算类型的指令*，比如加减乘除、位运算、比较大小等等，它们最多只能处理两个寄存器中的数据；
- *跳转类型的指令*，通过修改程序计数器的值来达到跳转执行指令的过程，比如编程中常见的 `if-else`、`switch-case`、函数调用等。
- *信号类型的指令*，比如发生中断的指令 `trap`；
- *闲置类型的指令*，比如指令 `nop`，执行后 CPU 会空转一个周期；



## · 指令的执行速度：

CPU 的硬件参数都会有 `GHz` 这个参数，比如一个 1 GHz 的 CPU，指的是**时钟频率**是 1 G，代表着 1 秒会产生 1G 次数的脉冲信号，每**一次脉冲信号高低电平的转换**就是**一个周期**，称为**时钟周期**。

对于 CPU 来说，在一个时钟周期内，CPU 仅能完成一个最基本的动作，**时钟频率越高，时钟周期就越短**，工作速度也就越快。

（1Ghz表示1秒有1G脉冲信号，一次高低电平的转换就是一个时钟周期，一个周期只能完成一个基本的动作）

~~~
一个时钟周期不一定能执行完成一条指令，大多数指令都需要若干时钟周期，乘法需要的时钟周期比加法要多；
~~~



### ·程序的执行速度：

·程序的执行速度快，则CPU时间少，CPU时间可被拆分为：

**CPU 时钟周期数（\*CPU Cycles\*）和时钟周期时间（\*Clock Cycle Time\*）的乘积**。

![image-20230327231341211](D:\typora笔记\操作系统笔记图片\image-20230327231341211.png)	

·时钟周期时间==CPU 主频，主频越高说明 CPU 的工作速度就越快；

·CPU 是 2.4 GHz 四核 Intel Core i5：

~~~
2.4 GHz 就是电脑的主频，时钟周期时间就是 1/2.4G（1s内产生2.4G脉冲信号）

要想 CPU 跑的更快，自然缩短时钟周期时间，也就是提升 CPU 主频，但是今非彼日，摩尔定律早已失效，当今的 CPU 主频已经很难再做到翻倍的效果了。
~~~

时钟周期时间是工程师无法控制的，此时需要往另外一个乘法因子下手，也就是时钟周期数；

【CPU 时钟周期数】可以进一步拆解成：「**指令数 x 每条指令的平均时钟周期数（\*Cycles Per Instruction\*，简称 `CPI`）**」

​	![image-20230327232153211](D:\typora笔记\操作系统笔记图片\image-20230327232153211.png)

- *指令数*，表示执行程序所需要多少条指令，以及哪些指令。这个层面是基本**靠编译器来优化**，毕竟同样的代码，在不同的编译器，编译出来的计算机指令会有各种不同的表示方式。
- *每条指令的平均时钟周期数 CPI*，表示一条指令需要多少个时钟周期数，现代大多数 CPU 通过**流水线技术**（Pipeline），让一条指令需要的 **CPU 时钟周期数**尽可能的少；
- *时钟周期时间*，表示计算机主频，取决于计算机硬件。有的 CPU 支持超频技术，打开了超频意味着把 CPU 内部的时钟给调快了，于是 CPU 工作速度就变快了，但是也是有代价的，CPU 跑的越快，散热的压力就会越大，CPU 会很容易奔溃。



## ·第一节总结：

~~~
通常来说 64 位 CPU 的地址总线是 48 位，而 32 位 CPU 的地址总线是 32 位，所以 64 位 CPU 可以寻址更大的物理内存空间。

如果一个 32 位 CPU 的地址总线是 32 位，那么该 CPU 最大寻址能力是 4G，即使你加了 8G 大小的物理内存，也还是只能寻址到 4G 大小的地址，

而如果一个 64 位 CPU 的地址总线是 48 位，那么该 CPU 最大寻址能力是 2^48，远超于 32 位 CPU 最大寻址能力。
~~~

### ·32位与64位软件的区别：

- 如果 32 位指令在 64 位机器上执行，需要一套兼容机制，就可以做到兼容运行了。但是**如果 64 位指令在 32 位机器上执行，就比较困难了，因为 32 位的寄存器存不下 64 位的指令**；
- 操作系统分为32，64位，64位系统的指令也是64位，无法在32位的系统上运行；

**硬件的 64 位和 32 位指的是 CPU 的位宽，软件的 64 位和 32 位指的是指令的位宽。**



## 

## ·硬盘与内存的速度差异：

![image-20230328114019394](D:\typora笔记\操作系统笔记图片\image-20230328114019394.png)	

### ·存储器的结构层次：

·可以分为三类：

1.CPU缓存；2.内存；3.SDD/HDD硬盘；

**·以图书馆为例说明：**

·CPU缓存类似于大脑现在正在阅读的内容，与大脑交互速度极快，但容量极小，能记住的不多；

·内存类似于书桌，将之后要读的数先放在书桌上，方便之后拿取阅读；

·SDD/HDD硬盘类似于图书馆的书架，暂时不需要阅读的书籍就先放在较远的书架上，等需要阅读时，再从书架取出放到书桌上，这就是从硬盘读取数据到内存的过程。

![image-20230328120036985](D:\typora笔记\操作系统笔记图片\image-20230328120036985.png)	

·L1缓存与CPU距离最近，被分为【指令缓存】，【数据缓存】两块区域。存储空间极小，因此类似于短期记忆；

·L2，3缓存存储空间相对较大，能长期记忆；

~~~
对于存储器而言，存储速度越快，能耗越高，材料成本越贵；
~~~

![image-20230328120413831](D:\typora笔记\操作系统笔记图片\image-20230328120413831.png)	

![image-20230328120424649](D:\typora笔记\操作系统笔记图片\image-20230328120424649.png)	

~~~
寄存器；
CPU Cache；
  L1-Cache；
  L2-Cache；
  L3-Cahce；
内存；
SSD/HDD 硬盘
~~~

### 寄存器：

~~~
最靠近CPU控制单元-逻辑单元的存储器，速度最快，容量最小，价格最高；
·32位的cpu，寄存器能存储4个字节，64位CPU寄存器能存储8个字节；
~~~

·寄存器的访问速度非常快，一般要求在半个 CPU 时钟周期（时间）内完成读写，CPU 时钟周期（时间）跟 CPU 主频息息相关，比如 2 GHz 主频的 CPU，那么它的时钟周期（时间）就是 1/2G，也就是 0.5ns（纳秒）。寄存器的读写速度过慢会拉长CPU时钟周期时间，导致程序执行速度变慢；

·CPU Cache使用SRAM（static random-access memory 静态随机存储）芯片；

#### **·静态：**

SRAM 之所以叫「静态」存储器，是因为只要有电，数据就可以保持存在，而一旦断电，数据就会丢失了。

~~~
在 SRAM 里面，一个 bit 的数据，通常需要 6 个晶体管，所以 SRAM 的存储密度不高，同样的物理空间下，能存储的数据是有限的，不过也因为 SRAM 的电路简单，所以访问速度非常快。
~~~

![image-20230328121635064](D:\typora笔记\操作系统笔记图片\image-20230328121635064.png)	

#### L1高速缓存：

·L1 高速缓存的访问速度几乎和寄存器一样快，通常只需要 `2~4` 个时钟周期，而大小在几十 KB 到几百 KB 不等。

·每个CPU核心都有自己的L1缓存，分为数据缓存+指令缓存；

**·Linux查看缓存大小：**

~~~sh
$ cat /sys/devices/system/cpu/cpu0/cache/index0/size #查看数据缓存
32K

$ cat /sys/devices/system/cpu/cpu0/cache/index1/size #查看指令缓存
32K
~~~



#### L2高速缓存：

·同样每个核心都有自己的L2缓存，容量更大，距离CPU更远，速度相比L1更慢；访问速度需要10~20个时钟周期；

~~~sh
$ cat /sys/devices/system/cpu/cpu0/cache/index2/size #查看二级缓存大小
256K
~~~



#### L3高速缓存：

·所有CPU核心共用的缓存，容量更大，距离CPU更远，速度更慢，需要20~60个时钟周期；

~~~sh
$ cat /sys/devices/system/cpu/cpu0/cache/index3/size #查看三级缓存大小
3072K
~~~



### 内存：

内存用的芯片和 CPU Cache 有所不同，它使用的是一种叫作 **DRAM （\*Dynamic Random Access Memory\*，动态随机存取存储器）** 的芯片。

·相比SRAM密度更高，容量更大，功耗更低，价格更低。

#### ·动态：

·原因：数据存储在晶体管和电容，电容会不断漏电，需要定时刷新；

~~~
DRAM 存储一个 bit 数据，只需要一个晶体管和一个电容就能存储，但是因为数据会被存储在电容里，电容会不断漏电，所以需要「定时刷新」电容，才能保证数据不会被丢失，这就是 DRAM 之所以被称为「动态」存储器的原因，只有不断刷新，数据才能被存储起来。
~~~

DRAM 的数据**访问电路和刷新电路**都比 SRAM **更复杂**，所以访问的速度会更慢，内存速度大概在 `200~300` 个 时钟周期之间。



#### ·SSD/HDD硬盘：

~~~
SSD（Solid-state disk） 就是我们常说的固体硬盘，结构和内存类似，但是它相比内存的优点是断电后数据还是存在的，而内存、寄存器、高速缓存断电后数据都会丢失。内存的读写速度比 SSD 大概快 10~1000 倍。

当然，还有一款传统的硬盘，也就是机械硬盘（Hard Disk Drive, HDD），它是通过物理读写的方式来访问数据的，因此它访问速度是非常慢的，它的速度比内存慢 10W 倍左右
~~~



#### ·存储器层次：

·CPU并不会与所有存储器直接交互，只会与相邻的存储器交互；

![image-20230328123448119](D:\typora笔记\操作系统笔记图片\image-20230328123448119.png)	

**·当前小结：**

~~~
每个存储器只和相邻的一层存储器设备打交道，并且存储设备为了追求更快的速度，所需的材料成本必然也是更高，也正因为成本太高，所以 CPU 内部的寄存器、L1\L2\L3 Cache 只好用较小的容量，相反内存、硬盘则可用更大的容量，这就我们今天所说的存储器层次结构。
~~~



#### ·存储器访问顺序：

~~~
1.当 CPU 需要访问内存中某个数据的时候，如果寄存器有这个数据，CPU 就直接从寄存器取数据即可，

2.如果寄存器没有这个数据，CPU 就会查询 L1 高速缓存，

3.如果 L1 没有，则查询 L2 高速缓存，

4.L2 还是没有的话就查询 L3 高速缓存，

5.L3 依然没有的话，才去内存中取数据。
~~~

·存储层次结构形成了缓存的体系：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/%E7%BC%93%E5%AD%98%E4%BD%93%E7%B3%BB1.png)	

·存储器价格和性能差异（了解）：

![image-20230328124221960](D:\typora笔记\操作系统笔记图片\image-20230328124221960.png)	

L1 Cache 的访问延时是 1 纳秒，而内存已经是 100 纳秒了，相比 L1 Cache 速度慢了 `100` 倍。另外，机械硬盘的访问延时更是高达 10 毫秒，相比 L1 Cache 速度慢了 `10000000` 倍，差了好几个数量级别。

在价格上，每生成 MB 大小的 L1 Cache 相比内存贵了 `466` 倍，相比机械硬盘那更是贵了 `175000` 倍。



### ·第二节小结：

机械硬盘、固态硬盘、内存这三个存储器，到底和 `CPU L1 Cache` 相比速度差多少倍

~~~
CPU L1 Cache 随机访问延时是 1 纳秒，内存则是 100 纳秒，所以 CPU L1 Cache 比内存快 100 倍左右。

SSD 随机访问延时是 150 微秒，所以 CPU L1 Cache 比 SSD 快 150000 倍左右。

最慢的机械硬盘随机访问延时已经高达 10 毫秒，我们来看看机械硬盘到底有多「龟速」：

    SSD 比机械硬盘快 70 倍左右；
    内存比机械硬盘快 100000 倍左右；
    CPU L1 Cache 比机械硬盘快 10000000 倍左右；
    

##时间比例放大：
 CPU 访问 L1 Cache 的缓存时间是 1 秒，那访问内存则需要大约 2 分钟，随机访问 SSD 里的数据则需要 1.7 天，访问机械硬盘那更久，长达近 4 个月。
~~~

**·内存分级的目的/意义：构建缓存体系；**



## 

## 写出让CPU运行更快的代码：

![image-20230329174716156](D:\typora笔记\操作系统笔记图片\image-20230329174716156.png)	

### ·CPU Cache的作用：

~~~
为了弥补CPU与内存之间的速度差异；

·原因（了解）：（只需要知道CPU的速度与内存的访问速度在不断拉大）
根据摩尔定律，CPU 的访问速度每 18 个月就会翻倍，相当于每年增长 60% 左右，内存的速度当然也会不断增长，但是增长的速度远小于 CPU，平均每年只增长 7% 左右。于是，CPU 与内存的访问性能的差距不断拉大。
~~~

### ·访问各个cache的速度：

![image-20230329175357298](D:\typora笔记\操作系统笔记图片\image-20230329175357298.png)	

·**CPU 从 L1 Cache 读取数据的速度，相比从内存读取的速度，会快 `100` 多倍。**



### ·Cache Line 缓存块：

~~~
·CPU缓存是由多个Cache Line组成的，Cache Line是CPU读取内存的基本单位。
·由各种标志（Tag）+ 数据块（Data Block）组成
~~~

![image-20230329180833802](D:\typora笔记\操作系统笔记图片\image-20230329180833802.png)	

·CPU Cache 的数据是从内存中读取过来的，它是以**一小块一小块读取数据**的，而不是按照单个数组元素来读取数据的，在 CPU Cache 中的，这样一小块一小块的数据，称为 **Cache Line（缓存块）**。

（内存中的一小块数据被读取到CPU缓存后，被称为缓存块）



### ·查看CPU缓存的缓存块大小：

![image-20230329181019607](D:\typora笔记\操作系统笔记图片\image-20230329181019607.png)	

**·案例：**

~~~
有一个 int array[100] 的数组，当载入 array[0] 时，由于这个数组元素的大小在内存只占 4 字节，不足 64 字节，CPU 就会顺序加载数组元素到 array[15]，意味着 array[0]~array[15] 数组元素都会被缓存在 CPU Cache 中了，因此当下次访问这些数组元素时，会直接从 CPU Cache 读取，而不用再从内存中读取，大大提高了 CPU 读取数据的性能。
~~~

### ·缓存访问机制：

![image-20230329181120526](D:\typora笔记\操作系统笔记图片\image-20230329181120526.png)

·此机制与使用「内存作为硬盘的缓存」的逻辑是一样的；



### ·CPU如何知道缓存的数据在缓存块中？

 CPU 访问内存数据时，是一小块一小块数据读取的，具体这一小块数据的大小，取决于 **coherency_line_size**的值，一般 64 字节。在内存中，这一块的数据我们称为**内存块（\*Block\*）**，读取的时候我们要拿到数据所在内存块的地址。

### ·直接映射cache：

~~~
对于直接映射 Cache 采用的策略，就是把内存块的地址始终「映射」在一个 CPU Cache Line（缓存块） 的地址，至于映射关系实现方式，则是使用「取模运算」，取模运算的结果就是内存块地址对应的 CPU Cache Line（缓存块） 的地址。

（就是说内存中的一个内存块始终映射着一块cpu的缓存块，通过取模运算的方式）
（取模运算的结果就是【内存块地址】对应的 CPU Cache Line（缓存块） 的【地址】）
~~~

**案例：**

内存共被划分为 32 个内存块，CPU Cache 共有 8 个 CPU Cache Line，假设 CPU 想要访问第 15 号内存块，如果 15 号内存块中的数据已经缓存在 CPU Cache Line 中的话，则是一定映射在 7 号 CPU Cache Line 中，因为 `15 % 8` 的值是 7。

（此时存在多个内存块都映射到同一个缓存块）

![image-20230329181803851](D:\typora笔记\操作系统笔记图片\image-20230329181803851.png)	

·此时需要区分不同内存块：通过【缓存块】的Tag来进行区分：

~~~
Tag记录当前 CPU Cache Line 中存储的数据对应的内存块，我们可以用这个组标记来区分不同的内存块。
~~~

#### **·Cache Line中存放的另外两个信息：**

- 一个是，从内存加载过来的实际存放**数据（\*Data\*）**。
- 另一个是，**有效位（\*Valid bit\*）**，它是用来标记对应的 CPU Cache Line 中的**数据是否是有效**的，如果**有效位是 0**，无论 CPU Cache Line 中是否有数据，CPU 都会直接访问内存，重新加载数据。

CPU 在从 CPU Cache 读取数据的时候，并不是读取 CPU Cache Line 中的整个数据块，而是读取 CPU 所需要的一个数据片段，这样的数据统称为一个**字（\*Word\*）**。通过一个**偏移量（Offset）**来找到CPU要读取的**“字”**。



#### **·内存访问地址：**

·一个内存的访问地址，包括**组标记、CPU Cache Line 索引、偏移量**这三种信息

·CPU通过这三类信息，找到CPU cache中缓存的数据；



**·CPU cache数据结构：**

~~~
索引 + 有效位 + 组标记 + 数据块组成。
~~~

![image-20230329190644812](D:\typora笔记\操作系统笔记图片\image-20230329190644812.png)	

·CPU根据内存地址中包含的3个信息，找到该数据在CPU缓存中的位置；没有命中再通过该地址去访问内存；

**·命中cpu cache的处理方式：**

1. 根据内存地址中索引信息，计算在 CPU Cache 中的索引，也就是找出对应的 CPU Cache Line 的地址；
2. 找到对应 CPU Cache Line 后，判断 CPU Cache Line 中的有效位，确认 CPU Cache Line 中数据是否是有效的，如果是无效的，CPU 就会直接访问内存，并重新加载数据，如果数据有效，则往下执行；
3. 对比内存地址中组标记和 CPU Cache Line 中的组标记，确认 CPU Cache Line 中的数据是我们要访问的内存数据，如果不是的话，CPU 就会直接访问内存，并重新加载数据，如果是的话，则往下执行；
4. 根据内存地址中偏移量信息，从 CPU Cache Line 的数据块中，读取对应的字。

~~~
小结：
1.找到索引对于的CPU缓存地址；
2.确定数据是否有效；
3.确定组标记是否为CPU当前要访问的内存块；
4.读取数据；
~~~

#### ·其他策略方式（了解）：

·除了直接映射 Cache 之外，还有其他通过内存地址找到 CPU Cache 中的数据的策略，比如全相连 Cache （*Fully Associative Cache*）、组相连 Cache （*Set Associative Cache*）等，这几种策策略的数据结构都比较相似，我们理解了直接映射 Cache 的工作方式，其他的策略如果你有兴趣去看，相信很快就能理解的了。





### ·写出让 CPU 跑得更快的代码：

~~~
·访问的数据在 CPU Cache 中的话，意味着缓存命中，缓存命中率越高的话，代码的性能就会越好，CPU 也就跑的越快。
·「如何写出让 CPU 跑得更快的代码？」这个问题，可以改成「如何写出 CPU 缓存命中率高的代码？」
~~~

#### ·优化思路：

~~~
 L1 Cache 通常分为「数据缓存」和「指令缓存」，这是因为 CPU 会分别处理数据和指令，比如 1+1=2 这个运算，+ 就是指令，会被放在「指令缓存」中，而输入数字 1 则会被放在「数据缓存」里。

因此，我们要分开来看「数据缓存」和「指令缓存」的缓存命中率。
~~~

#### ·代码示例（提升数据缓存的命中率）：

![image-20230329220140048](D:\typora笔记\操作系统笔记图片\image-20230329220140048.png)	

·形式1的效率会比形式2快很多倍；

**·原因：**形式1大大提高了缓存的命中率；

![image-20230329220312071](D:\typora笔记\操作系统笔记图片\image-20230329220312071.png)	

~~~
形式一用 array[i][j] 访问数组元素的顺序，正是和内存中数组元素存放的顺序一致。

当 CPU 访问 array[0][0] 时，由于该数据不在 Cache 中，于是会「顺序」把跟随其后的 3 个元素从内存中加载到 CPU Cache，这样当 CPU 访问后面的 3 个数组元素时，就能在 CPU Cache 中成功地找到数据，这意味着缓存命中率很高，缓存命中的数据不需要访问内存，这便大大提高了代码的性能。
~~~



·形式2的遍历方式：

![image-20230329220442645](D:\typora笔记\操作系统笔记图片\image-20230329220442645.png)	

·这种访问方式是跳跃式的，在内存中并没有按照连续的内存空间访问。如果N的数值很大，操作 `array[j][i]` 时，是没办法把 `array[j+1][i]` 也读入到 CPU Cache 中的，此时CPU就会去访问内存；



#### ·在访问`array[0][0]` 元素时，CPU 具体会一次从内存中加载多少元素到 CPU Cache 呢？

跟 CPU Cache Line 有关，它表示 **CPU Cache 一次性能加载数据的大小**，可以在 Linux 里通过 `coherency_line_size` 配置查看 它的大小，通常是 64 个字节。

![image-20230329220920272](D:\typora笔记\操作系统笔记图片\image-20230329220920272.png)	

·这也就意味着当访问 `array[0][0]` 时，由于该元素不足 64 字节，于是就会往后**顺序**读取 `array[0][0]~array[0][15]` 到 CPU Cache 中。顺序访问的 `array[i][j]` 因为利用了这一特点，所以就会比跳跃式访问的 `array[j][i]` 要快。

**因此，遇到这种遍历数组的情况时，按照内存布局顺序访问，将可以有效的利用 CPU Cache 带来的好处，这样我们代码的性能就会得到很大的提升**





#### ·CPU的分支预测器：

~~~
在了解接下来的优化前，需要了解这个结构；

对于 if 条件语句，意味着此时至少可以选择跳转到两段不同的指令执行，也就是 if 还是 else 中的指令。那么，如果分支预测可以预测到接下来要执行 if 里的指令，还是 else 指令的话，就可以「提前」把这些指令放在指令缓存中，这样 CPU 可以直接从 Cache 读取到指令，于是执行速度就会很快。

当数组中的元素是随机的，分支预测就无法有效工作，而当数组元素都是是顺序的，分支预测器会动态地根据历史命中数据对未来进行预测，这样命中率就会很高。
~~~





#### ·提升指令缓存的命中率：

**·代码示例：**

![image-20230329221129560](D:\typora笔记\操作系统笔记图片\image-20230329221129560.png)	

![image-20230329221137938](D:\typora笔记\操作系统笔记图片\image-20230329221137938.png)	

- 第一个操作，循环遍历数组，把小于 50 的数组元素置为 0；
- 第二个操作，将数组排序；

·根据分支预测器，可知：

~~~
因此，先排序再遍历速度会更快，这是因为排序之后，数字是从小到大的，那么前几次循环命中 if < 50 的次数会比较多，于是分支预测就会缓存 if 里的 【 array[i] = 0 】指令到 Cache 中，后续 CPU 执行该指令就只需要从 Cache 读取就好了。
~~~



#### ·C++的优化方式：

~~~
果你肯定代码中的 if 中的表达式判断为 true 的概率比较高，我们可以使用显示分支预测工具，比如在 C/C++ 语言中编译器提供了 likely 和 unlikely 这两种宏，如果 if 条件为 ture 的概率大，则可以用 likely 宏把 if 里的表达式包裹起来，反之用 unlikely 宏。
~~~

![image-20230329221702620](D:\typora笔记\操作系统笔记图片\image-20230329221702620.png)	

实际上，CPU 自身的动态分支预测已经是比较准的了，所以只有当非常确信 CPU 预测的不准，且能够知道实际的概率情况时，才建议使用这两种宏。





### 提升多核 CPU 的缓存命中率：

·在单核 CPU，虽然只能执行一个线程，但是操作系统给每个线程分配了一个时间片，时间片用完了，就调度下一个线程，于是各个线程就按时间片交替地占用 CPU，从宏观上看起来各个线程同时在执行。

~~~
现代 CPU 都是多核心的，线程可能在不同 CPU 核心来回切换执行，这对 CPU Cache 不是有利的，虽然 L3 Cache 是多核心之间共享的，但是 L1 和 L2 Cache 都是每个核心独有的，

如果一个线程在不同核心来回切换，各个核心的缓存命中率就会受到影响，相反如果【线程都在同一个核心上执行】，那么其数据的 L1 和 L2 Cache 的缓存命中率可以得到有效提高，缓存命中率高就意味着 CPU 可以减少访问 内存的频率。
~~~

当有多个同时执行「计算密集型」的线程，为了防止因为切换到不同的核心，而导致缓存命中率下降的问题，我们可以把**线程绑定在某一个 CPU 核心上**，这样性能可以得到非常可观的提升。



#### **·线程绑定CPU核心：**

在 Linux 上提供了 `sched_setaffinity` 方法，来实现将线程绑定到某个 CPU 核心这一功能。

![image-20230329222042056](D:\typora笔记\操作系统笔记图片\image-20230329222042056.png)	





## ·CPU缓存一致性：

![image-20230330191141933](D:\typora笔记\操作系统笔记图片\image-20230330191141933.png)	

### ·CPU Cache 的数据写入：

·数据写入cpu缓存之后，此时缓存数据与内存不一致，需要进行CPU缓存与内存的数据同步；

·CPU缓存数据写回内存的方式有两种：

~~~
写直达（Write Through）
写回（Write Back）
~~~

### ·写直达：

**把数据同时写入内存和 Cache 中**，这种方法称为**写直达（\*Write Through\*）**。

![image-20230330192810983](D:\typora笔记\操作系统笔记图片\image-20230330192810983.png)	

在这个方法里，写入前会先判断数据是否已经在 CPU Cache 里面了：

- 如果数据已经在 Cache 里面，先将数据更新到 Cache 里面，再写入到内存里面；
- 如果数据没有在 Cache 里面，就直接把数据更新到内存里面。

写直达法很直观，也很简单，但是问题明显，无论数据在不在 Cache 里面，**每次写操作都会写回到内存**，这样写操作将会花费大量的时间，无疑**性能会受到很大的影响**

### ·写回：

·【写回】方式在写直达的基础上进行优化，减少数据写回内存的频率；

·**当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中**，减少了数据写回内存的频率，这样便可以提高系统的性能。

~~~
AI回答：

CPU cache的写回操作是指当CPU向高速缓存中写入数据时，如果该数据原本在高速缓存中存在并已经被修改过，那么在将数据写入高速缓存之前，需要先将修改后的数据写回到主存中。

当CPU需要访问数据时，它首先会在高速缓存中查找该数据是否存在。如果数据已经在高速缓存中，CPU可以直接从高速缓存中读取数据，从而提高访问速度。当CPU对数据进行修改时，修改的数据会被写入高速缓存中，并且标记该数据已经被修改过。这时，如果有其他数据需要访问该数据，它将会访问到被修改后的数据，而不是原来的数据。

当CPU写入数据时，如果该数据在高速缓存中已经存在并且被修改过，那么在将数据写入高速缓存之前，需要先将被修改后的数据写回到主存中，从而保证数据的一致性。这种写回操作可以避免出现高速缓存中数据和主存中数据不一致的问题，从而保证数据的正确性和可靠性。

需要注意的是，写回操作会带来一定的性能损失，因为每次写回操作都需要将数据从高速缓存中写回到主存中，这需要消耗一定的时间和资源。为了避免频繁进行写回操作，CPU cache通常采用写回策略和写通过策略相结合的方式。在写回策略下，数据被修改后并不会立即写回主存，而是将其标记为已修改并暂存到高速缓存中。只有当该数据被替换出高速缓存或者CPU需要读取该数据时，才会将数据写回主存。而在写通过策略下，数据修改后立即被写回主存，这样可以避免出现数据不一致的问题，但会带来一定的性能损失。
~~~

~~~
小结：当数据往CPU缓存写入时，会先检查该数据是否已经（在之前）就被保存到cpu缓存中，如果已经被保存过，则会将修改后（新的数据）写入该cpu cache block中，并且标记【这个缓存块已经被修改过了（脏的）】；

·在下一次又需要往这块被标记为（脏的）的缓存块写入数据时，cpu会执行如下操作：
1.先将当前将要写入的，这块缓存块的数据，写入到内存中；
2.数据写入内存后，再将内存中的数据读取到缓存块中；
3.最后才将最新的数据写入到缓存块中，并且标记这块缓存块是被修改过的（脏的）
~~~

·为什么需要第2个步骤？将内存的数据重新写入缓存块中，按理说此时的缓存块的数据就是最新的数据：

~~~
AI回答：
在写回操作中，为什么需要重新将内存中的数据写入高速缓存中呢？原因是高速缓存中的数据可能不是最新的数据，这是因为在高速缓存中的数据被修改后，相应的内存地址中的数据可能也被其他CPU核心修改过了，所以在执行写回操作之前，需要先将内存中的最新数据读取到高速缓存中，以确保高速缓存中的数据是最新的。

如果不将内存中的最新数据读取到高速缓存中，那么在高速缓存中的数据可能就是过期的，这就可能导致在下一次对这个地址进行读取或写入操作时，CPU核心从高速缓存中读取到的是过期的数据，从而引发程序错误或异常。

因此，为了确保高速缓存中的数据是最新的，需要在写回操作之前，将内存中的最新数据读取到高速缓存中，并且更新高速缓存中的标志位，以便在下一次对这个地址进行操作时，能够正确地判断数据是否已经过期。

·小结：为了避免缓存中的数据不是最新的数据；
~~~



### ·总线嗅探：

#### ·写传播：（总线嗅探）

~~~
当某个 CPU 核心更新了 Cache 中的数据，要把该事件广播通知到其他核心。最常用的方式就是总线嗅探。
~~~

#### ·举例说明：

·当A号CPU核心改变了L1缓存中 i 变量的值，此时需要通过总线八这个改变事件通知给其他的CPU核心，这时每个CPU核心都会监听总线上的广播事件，并且检查自己的L1缓存中是否存在相同的数据，例如此时B号CPU核心的L1缓存中也存有 i 这个变量，则B核心需要根据广播事件同步修改自己缓存中的 i 变量。

#### 缺点：

~~~
1.CPU 需要每时每刻监听总线上的一切活动，但是不管别的核心的 Cache 是否缓存相同的数据，都需要发出一个广播事件，这无疑会加重总线的负载。
2.总线嗅探只是保证了某个 CPU 核心的 Cache 更新数据这个事件能被其他 CPU 核心知道，但是并不能保证事务串行化。
~~~

#### 解决方案：

~~~
MESI 协议：基于总线嗅探机制实现了【事务串行化】，用【状态机】机制降低了【总线带宽压力】。做到了 CPU 缓存一致性。
~~~



###  MESI 协议：

·来源四个单词的缩写：

- *Modified*，已修改
- *Exclusive*，独占
- *Shared*，共享
- *Invalidated*，已失效

**四个状态来标记 Cache Line 四个不同的状态。**



#### ·已修改：

~~~
表示当前CPU缓存已经被修改过，被标记为【脏的】，还没有写入内存中；
~~~

#### ·已失效：

~~~
表示当前的cpu cache block中的数据已经失效了，不可读取；
~~~

#### ·共享--独享：

~~~
两者都表示当前CPU缓存和内存是缓存一致的状态，区别在于：
	【独享】：表示当前数据只单独存在于一个CPU核心中，其他CPU核心都没有，因此要修改这个数据时直接修改这个核心缓存即可，不需要广播通知，此时就不存在缓存一致性的问题了，因为只有一个CPU核心能操作该数据。
	
	【共享】：在独享概念的基础上，当其他CPU核心从内存读取到了相同的数据，此时这个数据就不再是独享的了，会被改变为共享的状态。此时如果要修改共享的数据，不能直接修改核心，需要先发布一个广播通知其他CPU核心，并且标记【含有相同数据】的其他CPU核心的 cache line 为无效状态，避免修改期间该数据被读取。最后才修改cache中的数据。
~~~

#### ·四个状态转换（案例说明）：

1. 当 A 号 CPU 核心从内存读取变量 i 的值，数据被缓存在 A 号 CPU 核心自己的 Cache 里面，此时其他 CPU 核心的 Cache 没有缓存该数据，于是标记 Cache Line 状态为「独占」，此时其 Cache 中的数据与内存是一致的；
2. 然后 B 号 CPU 核心也从内存读取了变量 i 的值，此时会发送消息给其他 CPU 核心，由于 A 号 CPU 核心已经缓存了该数据，所以会把数据返回给 B 号 CPU 核心。在这个时候， A 和 B 核心缓存了相同的数据，Cache Line 的状态就会变成「共享」，并且其 Cache 中的数据与内存也是一致的；
3. 当 A 号 CPU 核心要修改 Cache 中 i 变量的值，发现数据对应的 Cache Line 的状态是共享状态，则要向所有的其他 CPU 核心广播一个请求，要求先把其他核心的 Cache 中对应的 Cache Line 标记为「无效」状态，然后 A 号 CPU 核心才更新 Cache 里面的数据，**同时标记 Cache Line 为「已修改」状态**（此时其他核心中对应的Cache Line是无效状态），此时 Cache 中的数据就与内存不一致了。
4. 如果 A 号 CPU 核心「继续」修改 Cache 中 i 变量的值，由于此时的 Cache Line 是「已修改」状态（此时其他核心中对应的Cache Line是无效状态），因此不需要给其他 CPU 核心发送消息，直接更新数据即可。
5. 如果 A 号 CPU 核心的 Cache 里的 i 变量对应的 Cache Line 要被「替换」，发现 Cache Line 状态是「已修改」状态，就会在替换前先把数据同步到内存。



#### ·MESI协议相比总线嗅探的优化：

~~~
当 Cache Line 状态是「已修改」或者「独占」状态时，修改更新其数据不需要发送广播给其他 CPU 核心，这在一定程度上减少了总线带宽压力。
~~~

#### ·状态机：

MESI 的状态可以用一个有限状态机来表示它的状态流转；对于不同状态触发的事件操作，可能是来自本地 CPU 核心发出的广播事件，也可以是来自其他 CPU 核心通过总线发出的广播事件：

![image-20230409120013206](D:\typora笔记\操作系统笔记图片\image-20230409120013206.png)	

#### ·状态机图的解读方式：

**·失效状态的四个事件（对应上面的【已失效方块】四个箭头）**

![image-20230409120454875](D:\typora笔记\操作系统笔记图片\image-20230409120454875.png)	

·独占和共享状态的四个事件：（同理对应上面的箭头理解）

![image-20230409120630649](D:\typora笔记\操作系统笔记图片\image-20230409120630649.png)	

·已修改状态的四个事件：

![image-20230409120719329](D:\typora笔记\操作系统笔记图片\image-20230409120719329.png)	

### ·小结：

1.CPU在读写数据时，都是从cpu cache读取数据，因为离cpu很近。如果cache中没有数据，就从内存中读取数据，同时将内存中读到的数据写入cache中。

2.数据的写入有两种方式：「写直达」和「写回」这两种策略来保证 Cache 与内存的数据一致性；

- 写直达，只要有数据写入，都会直接把数据写入到内存里面，这种方式简单直观，但是性能就会受限于内存的访问速度；
- 写回，对于已经缓存在 Cache 的数据的写入，只需要更新其数据就可以，不用写入到内存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到内存里，这种方式在缓存命中率高的情况，性能会更好；

CPU 都是多核的，每个核心都有各自独立的 L1/L2 Cache，只有 L3 Cache 是多个核心之间共享的。所以，我们要确保多核缓存是一致性的，否则会出现错误的结果。

**3.实现缓存一致性，关键是要满足 2 点：**

- 第一点是**写传播**，也就是当某个 CPU 核心发生写入操作时，需要把该事件广播通知给其他核心；
- 第二点是**事物的串行化**，这个很重要，只有保证了这个，才能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的；

基于总线嗅探机制的 MESI 协议，就满足上面了这两点，因此它是保障缓存一致性的协议。

MESI 协议，是已修改、独占、共享、已失效这四个状态的英文缩写的组合。整个 MSI 状态的变更，则是根据来自本地 CPU 核心的请求，或者来自其他 CPU 核心通过总线传输过来的请求，从而构成一个流动的状态机。另外，**对于在「已修改」或者「独占」状态的 Cache Line，修改更新其数据不需要发送广播给其他 CPU 核心**。

------

## 

## ·CPU如何执行任务：

![image-20230409124422470](D:\typora笔记\操作系统笔记图片\image-20230409124422470.png)	

### ·CPU伪共享：

#### ·CPU架构：

·数据缓存dcache，指令缓存icache；

![image-20230409124646841](D:\typora笔记\操作系统笔记图片\image-20230409124646841.png)	

·CPU 从内存中读取数据到 Cache 的时候，并不是一个字节一个字节读取，而是一块一块的方式来读取数据的，这一块一块的数据被称为 CPU Cache Line（缓存块），所以 **CPU Cache Line 是 CPU 从内存读取数据到 Cache 的【单位】**。

~~~java
当读取数组时，会读取一串连续内存地址的多个数据；但是如果不再使用数组，而是使用单个变量时，会出现CPU伪共享问题。

//【性能杀手，需要规避】//
~~~



#### ·伪共享：

**·案例说明：**

假设有一个双核心的 CPU，这两个 CPU 核心并行运行着两个不同的线程，它们同时从内存中读取两个不同的数据，分别是类型为 `long` 的变量 A 和 B，这个两个数据的地址在**【物理内存】**上是**连续**的，如果 Cahce Line 的大小是 64 字节，并且变量 A 在 Cahce Line 的开头位置，那么这两个数据是位于**同一个 Cache Line 中**，又因为 CPU Cache Line 是 CPU 从内存读取数据到 Cache 的**单位**，所以这两个数据会被同时读入到了两个 CPU 核心中各自 Cache 中。

![image-20230409125057552](D:\typora笔记\操作系统笔记图片\image-20230409125057552.png)	

**·问题：**

如果这两个不同核心的线程分别修改不同的数据，比如 1 号 CPU 核心的线程只修改了 变量 A，或 2 号 CPU 核心的线程的线程只修改了变量 B，会发生什么呢？



### ·伪共享问题分析：

①. 最开始变量 A 和 B 都还不在 Cache 里面，

假设 ：

~~~
1 号核心绑定了线程 A，
2 号核心绑定了线程 B，

线程 A 只会读写变量 A，
线程 B 只会读写变量 B。
~~~

![image-20230409125659775](D:\typora笔记\操作系统笔记图片\image-20230409125659775.png)	

②. 1 号核心读取变量 A，由于 CPU 从内存读取数据到 Cache 的单位是 Cache Line，也正好变量 A 和 变量 B 的数据归属于同一个 Cache Line，所以 A 和 B 的数据都会被加载到 Cache，并将此 Cache Line 标记为**「独占」**状态。

~~~
此时只有1号核心读取了数据，因此目前A 和 B 的数据 由1核心独占；
~~~

![image-20230409125816623](D:\typora笔记\操作系统笔记图片\image-20230409125816623.png)	

③. 接着，2 号核心开始从内存里读取变量 B，同样的也是读取 Cache Line 大小的数据到 Cache 中，此 Cache Line 中的数据也包含了变量 A 和 变量 B，此时 1 号和 2 号核心的 Cache Line 状态变为**「共享」**状态。

~~~
1号，2号核心都读取了同一块数据，此时这块数据是两个核心共享的；
~~~

![image-20230409130220624](D:\typora笔记\操作系统笔记图片\image-20230409130220624.png)	

④. 1 号核心需要修改变量 A，发现此 Cache Line 的状态是「共享」状态，所以先需要通过总线发送消息给 2 号核心，通知 2 号核心把 Cache 中对应的 Cache Line 标记为「已失效」状态，然后 1 号核心对应的 Cache Line 状态变成「已修改」状态，并且修改变量 A。

~~~
此时核心1修改了数据，则核心2的数据就失效了，核心2想要再使用A，B变量，就需要重新从内存读取
~~~



![image-20230409130257680](D:\typora笔记\操作系统笔记图片\image-20230409130257680.png)	

⑤. 之后，2 号核心需要修改变量 B，此时 2 号核心的 Cache 中对应的 Cache Line 是已失效状态，另外由于 1 号核心的 Cache 也有此相同的数据，且状态为「已修改」状态，所以要先把 1 号核心的 Cache 对应的 Cache Line 写回到内存，然后 2 号核心再从内存读取 Cache Line 大小的数据到 Cache 中，最后把变量 B 修改到 2 号核心的 Cache 中，并将状态标记为「已修改」状态。

~~~
此时2号核心要修改B变量，从内存中重新读取数据；
2号核心修改后，1号核心的数据又变成了已失效状态，1号核心想要再使用，就需要再次从内存读取；
~~~

所以，可以发现如果 1 号和 2 号 CPU 核心这样持续交替的分别修改变量 A 和 B，就会重复 ④ 和 ⑤ 这两个步骤，Cache 并没有起到缓存的效果，虽然变量 A 和 B 之间其实并没有任何的关系，但是因为同时归属于一个 Cache Line ，这个 Cache Line 中的任意数据被修改后，都会相互影响，从而出现 ④ 和 ⑤ 这两个步骤。

~~~java
两个核心重复这个步骤，则缓存就没有起到作用。

/*因此，这种因为多个线程同时读写同一个 Cache Line 的不同变量时，而导致 CPU Cache 失效的现象称为伪共享（False Sharing）*/
~~~



### ·避免伪共享问题：

对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同一个 Cache Line 中，否则就会出现为伪共享的问题。

#### ·解决方案：

在 Linux 内核中存在 `__cacheline_aligned_in_smp` 宏定义，是用于解决伪共享的问题。

![image-20230409130753311](D:\typora笔记\操作系统笔记图片\image-20230409130753311.png)	

~~~c++
#ifdef CONFIG_SMP // 如果定义了 CONFIG_SMP
#define __cacheline_aligned_in_smp  __cacheline_aligned // 定义 __cacheline_aligned_in_smp cacheline_aligned 宏
#else // 否则
#define __cacheline_aligned_in_smp // 定义 _cacheline_aligned_in_smp 宏为空
#endif // 结束宏定义
~~~

- 如果在多核（MP）系统里，该宏定义是 `__cacheline_aligned`，也就是 Cache Line 的大小；
- 而如果在单核系统里，该宏定义是空的；

因此，针对在同一个 Cache Line 中的共享的数据，如果在多核之间竞争比较严重，为了防止伪共享现象的发生，可以采用上面的宏定义使得变量在 Cache Line 里是对齐的。

### ·案例说明：

·目前有一个结构体：

![image-20230412221157096](D:\typora笔记\操作系统笔记图片\image-20230412221157096.png)	

·结构体里的两个成员变量 a 和 b 在物理内存地址上是连续的，于是它们可能会位于同一个 Cache Line 中，如下图：

![image-20230412221219958](D:\typora笔记\操作系统笔记图片\image-20230412221219958.png)	

所以，为了防止前面提到的 Cache 伪共享问题，我们可以使用上面介绍的宏定义，将 b 的地址设置为 Cache Line 对齐地址，如下：

![image-20230412221238199](D:\typora笔记\操作系统笔记图片\image-20230412221238199.png)	

·此时a，b变量就不会在同一个cache line中：

![image-20230412221300463](D:\typora笔记\操作系统笔记图片\image-20230412221300463.png)	

~~~java
所以，避免 Cache 伪共享实际上是用/*空间换时间*/的思想，浪费一部分 Cache 空间，从而换来性能的提升。
~~~



### ·Disruptor 框架案例说明：

~~~
Java 并发框架 Disruptor 使用「字节填充 + 继承」的方式，来避免伪共享的问题。
~~~

![image-20230412223504922](D:\typora笔记\操作系统笔记图片\image-20230412223504922.png)	

·这里的三个类从上往下继承，根据 JVM 对象继承关系中父类成员和子类成员，**内存地址是连续排列布局**的；

~~~
RingBufferPad 类里 7 个 long 类型起到性能提升的作用；
~~~

·CPU Cache 从内存读取数据的单位是 CPU Cache Line，一般 64 位 CPU 的 CPU Cache Line 的大小是 64 个字节，一个 long 类型的数据是 8 个字节，所以 **CPU 一下会加载 8 个 long 类型的数据**。

·这三个类是继承关系，变量在内存地址中是**连续排列**的；

·RingBuffer 中的 7 个 long 类型数据则作为 Cache Line **后置填充**；

![image-20230412223804521](D:\typora笔记\操作系统笔记图片\image-20230412223804521.png)	

**·这里图示：**cpu一次读取了64字节，其中一个cache line里面有7个long数据类型占位置，因此只留下一部分空间给**RingBufferFelds** 里面定义的变量；

~~~
如果是64位的CPU，一次读取64字节，7个long数据类型占了56字节，剩下的8个字节用来存储填充对象的对象头信息¹。这样可以避免多个变量被存储在同一个缓存行中，从而避免了伪共享的问题¹³。

源: 与必应的对话， 2023/4/12
(1) CPU中的缓存、缓存一致性、伪共享和缓存行填充 - 知乎. https://zhuanlan.zhihu.com/p/135462276.
(2) 图解丨什么是伪共享？又该怎么避免伪共享的问题？ - 知乎. https://zhuanlan.zhihu.com/p/458926355.
(3) False share 伪共享-Cache line Padding 缓存行填充 缓存行对齐 - 知乎. https://zhuanlan.zhihu.com/p/339467137.
~~~



·这就保证了RingBufferFelds中定义的变量不会连续存储到同一个cache line；并且这些变量是final类型，之后不可能会被修改，**只要数据被频繁地读取访问，就不会出现数据被换出cache的可能，也因此不会产生伪共享的问题**。

**由于「前后」各填充了 7 个不会被读写的 long 类型变量，所以无论怎么加载 Cache Line，这整个 Cache Line 里都没有会发生更新操作的数据，但是生效的前提是，存储的对象必须满足大于等于8个字节，如果8个字节内存了两个能修改的对象，那这7个填充变量就没用了。**

~~~
RingBufferFelds是Disruptor中的一个类，里面定义的这些变量都是final修饰的，意味着第一次加载之后不会再修改。另外，由于「前后」各填充了7个不会被读写的long类型变量，所以无论怎么加载Cache Line，这整个Cache Line里都没有会发生更新操作的数据。这样可以避免伪共享的问题。

伪共享问题是由于多个线程访问同一个缓存行而导致的性能问题。当多个线程同时访问同一个缓存行时，由于缓存一致性协议的存在，CPU需要将该缓存行从其他CPU的缓存中移除，这样会导致其他CPU访问该缓存行时需要重新从内存中读取数据，从而降低了程序的性能。而填充一些不会被读写的变量可以避免多个线程访问同一个缓存行 。

~~~

·详细可查看这篇文章，简单易懂：

~~~
https://zhuanlan.zhihu.com/p/135462276
~~~



### ·CPU如何选择线程：

#### ·进程-线程：

linux中进程和线程都是使用`task_struct` 结构体表示。两者区别是线程的`task_struct`结构体中的部分资源是共享进程的已创建的资源。

例如内存地址空间、代码段、文件描述符等；因此线程也被称为轻量级进程，因为线程的 task_struct 相比进程的 task_struct 承载的 资源比较少，因此以「轻」得名。

没有创建线程的进程，是只有单个执行流，它被称为是**主线程**。如果想让进程处理更多的事情，可以创建多个线程分别去处理，但不管怎么样，它们**对应到内核里都是** `task_struct`。

![image-20230413014656072](D:\typora笔记\操作系统笔记图片\image-20230413014656072.png)	

·Linux 内核里的调度器，调度的对象就是 `task_struct`，接下来我们就把这个数据结构统称为**任务**。



#### ·任务优先级：

在 Linux 系统中，根据任务的优先级以及响应要求，主要分为两种，其中优先级的数值越小，优先级越高：

- **实时任务**，对系统的响应时间要求很高，也就是要尽可能快的执行实时任务，优先级在 `0~99` 范围内的就算实时任务；
- **普通任务**，响应时间没有很高的要求，优先级在 `100~139` 范围内都是普通任务级别；

#### ·调度类：

分为三种：

![image-20230413014845231](D:\typora笔记\操作系统笔记图片\image-20230413014845231.png)	

·**Deadline 和 Realtime 这两个调度类**，都是应用于实时任务的，这两个调度类的调度策略合起来共有这三种，它们的作用如下：

- *SCHED_DEADLINE*：是按照 deadline 进行调度的，距离当前时间点最近的 deadline 的任务会被优先调度；
- *SCHED_FIFO*：对于相同优先级的任务，按先来先服务的原则，但是**优先级更高的任务，可以抢占低优先级的任务**，也就是优先级高的可以「插队」；
- *SCHED_RR*：对于相同优先级的任务，轮流着运行，每个任务都有一定的时间片，当用完时间片的任务会被放到队列尾部，以保证相同优先级任务的公平性，但是**高优先级的任务依然可以抢占低优先级**的任务；

**Fair 调度类**是应用于普通任务，都是由 CFS 调度器管理的，分为两种调度策略：

- *SCHED_NORMAL*：普通任务使用的调度策略；
- *SCHED_BATCH*：后台任务的调度策略，不和终端进行交互，因此在不影响其他需要交互的任务，可以适当降低它的优先级



#### ·完全公平调度：

对于普通任务来说，公平性最重要，在 Linux 里面，实现了一个基于 CFS 的调度算法，也就是**完全公平调度（\*Completely Fair Scheduling\*）**。

~~~
分配给每个任务的 CPU 时间是一样，于是它为每个任务安排一个虚拟运行时间 vruntime，如果一个任务在运行，其运行的越久，该任务的 vruntime 自然就会越大，而没有被运行的任务，vruntime 是不会变化的。
~~~

**在 CFS 算法调度的时候，会优先选择 vruntime 少的任务**，以保证每个任务的公平性。

（让你把一桶的奶茶平均分到 10 杯奶茶杯里，你看着哪杯奶茶少，就多倒一些；哪个多了，就先不倒，这样经过多轮操作，虽然不能保证每杯奶茶完全一样多，但至少是公平的）



#### ·普通任务的优先级别：

普通任务之间是有优先级区分的，在计算虚拟运行时间 vruntime 还要考虑普通任务的**权重值**，注意**权重值并不是优先级的值**，内核中会有一个 **nice 级别与权重值的转换表**，**nice 级别越低的权重值就越大**；

![image-20230413015534723](D:\typora笔记\操作系统笔记图片\image-20230413015534723.png)	

·NICE_0_LOAD 可以认为是一个常量；不必在意这个值的含义；

~~~
在「同样的实际运行时间」里，高权重任务的 vruntime 比低权重任务的 vruntime 少，

 CFS 调度，会优先选择 vruntime 少的任务进行调度，所以高权重的任务就会被优先调度了，于是高权重的获得的实际运行时间自然就多了。
~~~



#### ·CPU运行队列：

·一个系统通常都会运行着很多任务，多任务的数量基本都是远超 CPU 核心数量，因此这时候就需要**排队**。

·每个 CPU 都有自己的**运行队列（\*Run Queue, rq\*）**，用于描述在此 CPU 上所运行的所有进程，其队列包含三个运行队列，Deadline 运行队列 dl_rq、实时任务运行队列 rt_rq 和 CFS 运行队列 cfs_rq，其中 **cfs_rq 是用红黑树**来描述的，**按 vruntime 大小来排序**的，**最左侧的叶子节点，就是下次会被调度的任务**。

![image-20230413020000133](D:\typora笔记\操作系统笔记图片\image-20230413020000133.png)	

#### ·队列调度优先级：

~~~java
优先级如下：Deadline > Realtime > Fair，这意味着 Linux 选择下一个任务执行的时候，会按照此优先级顺序进行选择，也就是说先从 dl_rq 里选择任务，然后从 rt_rq 里选择任务，最后从 cfs_rq 里选择任务。因此，/*实时任务总是会比普通任务优先被执行*/。
~~~

#### ·优先级调整：

**·nice值的作用：**

·作为调整普通任务优先级的修正数值；

~~~
启动任务的时候，没有特意去指定优先级的话，默认情况下都是普通任务，普通任务的调度类是 Fair，由 CFS 调度器来进行管理。CFS 调度器的目的是实现任务运行的公平性，也就是保障每个任务的运行的时间是差不多的。
~~~

想要某个普通任务有更多的执行时间，可以调整任务的 `nice` 值，从而让优先级高一些的任务执行更多时间。nice 的值能设置的范围是 `-20～19`， 值越低，表明优先级越高，因此 -20 是最高优先级，19 则是最低优先级，默认优先级是 0。

~~~
nice 值并不是表示优先级，而是表示优先级的修正数值，它与优先级（priority）的关系是这样的：priority(new) = priority(old) + nice。内核中，priority 的范围是 0~139，值越低，优先级越高，其中前面的 0~99 范围是提供给实时任务使用的，而 nice 值是映射到 100~139，这个范围是提供给普通任务用的，因此 nice 值调整的是普通任务的优先级。
~~~

![image-20230414022158944](D:\typora笔记\操作系统笔记图片\image-20230414022158944.png)		

#### 权重值与 nice 值的关系：

权重值与 nice 值的关系，nice 值越低，权重值就越大，计算出来的 vruntime 就会越少，由于 CFS 算法调度的时候，就会优先选择 vruntime 少的任务进行执行，所以 nice 值越低，任务的优先级就越高。

![image-20230414022330604](D:\typora笔记\操作系统笔记图片\image-20230414022330604.png)	

![image-20230414022400831](D:\typora笔记\操作系统笔记图片\image-20230414022400831.png)	



#### ·chrt指令：用于调整进程优先级

![image-20230413014202510](D:\typora笔记\操作系统笔记图片\image-20230413014202510.png)	

| 选项          | 描述                                    |
| ------------- | --------------------------------------- |
| -p, --pid     |                                         |
|               | 操作一个已存在的PID，不启动一个新的任务 |
| -b, --batch   | 设置调度策略为SCHED_BATCH               |
| -f, --fifo    |                                         |
|               | 设置调度策略为 SCHED_FIFO               |
| -m, --max     |                                         |
|               | 显示最小和最大有效优先级，然后退出      |
| -o, --other   |                                         |
|               | 设置策略调度策略为 SCHED_OTHER          |
| -r, --rr      |                                         |
|               | 设置调度策略为SCHED_RR(默认)            |
| -h, --help    |                                         |
|               | 显示使用信息并退出                      |
| -v, --version |                                         |
|               | 输出版本信息并退出                      |



## ·软中断：

![image-20230415020819487](D:\typora笔记\UE4笔记\笔记图片\image-20230415020819487.png)	

·中断，系统响应硬件设备请求的一种机制。操作系统收到硬件的中断请求，打断正在执行的进程。调用内核的中断处理程序来响应请求。

·中断是一种异步的事件处理机制。用于提高系统的并发处理能力。

**·案例说明：**

~~~
小林中午搬完砖，肚子饿了，点了份白切鸡外卖，这次我带闪了，没有被某团大数据杀熟。虽然平台上会显示配送进度，但是我也不能一直傻傻地盯着呀，时间很宝贵，当然得去干别的事情，等外卖到了配送员会通过「电话」通知我，电话响了，我就会停下手中地事情，去拿外卖。

这里的打电话，其实就是对应计算机里的中断，没接到电话的时候，我可以做其他的事情，只有接到了电话，也就是发生中断，我才会停下当前的事情，去进行另一个事情，也就是拿外卖。
~~~

### ·中断：

·操作系统收到了中断请求，会打断其他进程的运行，所以**中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度地影响。**

~~~
中断处理程序在响应中断时，会【临时关闭中断】。即当前中断程序没有被执行完，系统中的其他中断请求都不会被响应。这就意味着中断可能丢失。

·因此，中断程序需要处理的短且快。
~~~

**·案例说明：**

~~~
还是回到外卖的例子，小林到了晚上又点起了外卖，这次为了犒劳自己，共点了两份外卖，一份小龙虾和一份奶茶，并且是由不同地配送员来配送，那么问题来了，当第一份外卖送到时，配送员给我打了长长的电话，说了一些杂七杂八的事情，比如给个好评等等，但如果这时另一位配送员也想给我打电话。

很明显，这时第二位配送员因为我在通话中（相当于关闭了中断响应），自然就无法打通我的电话，他可能尝试了几次后就走掉了（相当于丢失了一次中断）。
~~~



### ·软中断：

·Linux 系统**为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是「上半部和下半部分」**。

#### ·上半部分：

~~~java
/*上半部用来快速处理中断*/，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。
~~~

#### ·下半部分：

~~~java
下半部用来延迟处理上半部未完成的工作，一般以/*「内核线程」*/的方式运行。
~~~



### ·案例说明：

~~~
前面的外卖例子，由于第一个配送员长时间跟我通话，则导致第二位配送员无法拨通我的电话，其实当我接到第一位配送员的电话，可以告诉配送员说我现在下楼，剩下的事情，等我们见面再说（上半部），然后就可以挂断电话，到楼下后，在拿外卖，以及跟配送员说其他的事情（下半部）。

这样，第一位配送员就不会占用我手机太多时间，当第二位配送员正好过来时，会有很大几率拨通我的电话。
~~~

**·案例二：**

网卡收到网络包后，通过 DMA 方式将接收到的数据写入内存，接着会通过**硬件中断**通知内核有新的数据到了，于是内核就会调用对应的中断处理程序来处理该事件，这个事件的处理也是会分成上半部和下半部。

上部分要做的事情很少，会先禁止网卡中断，避免频繁硬中断，而降低内核的工作效率。接着，内核会触发一个**软中断**，把一些处理比较耗时且复杂的事情，交给「软中断处理程序」去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。

所以，中断处理程序的上部分和下半部可以理解为：

- **上半部直接处理硬件请求，也就是硬中断**，主要是负责耗时短的工作，特点是快速执行；
- **下半部是由内核触发，也就说软中断**，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行；

还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且每一个 CPU 都对应一个软中断内核线程，名字通常为「ksoftirqd/CPU 编号」，比如 0 号 CPU 对应的软中断内核线程的名字是 `ksoftirqd/0`

不过，软中断不只是包括硬件设备中断处理程序的下半部，一些内核自定义事件也属于软中断，比如内核调度等、RCU 锁（内核里常用的一种锁）等。





### ·系统中的软中断：

在 Linux 系统里，我们可以通过查看 `/proc/softirqs` 的 内容来知晓「软中断」的运行情况，以及 `/proc/interrupts` 的 内容来知晓「硬中断」的运行情况。

![image-20230415023807210](D:\typora笔记\UE4笔记\笔记图片\image-20230415023807210.png)	

每一个 CPU 都有自己对应的不同类型软中断的【**累计运行次数**】，有 3 点需要注意下。

**·中断类型：**

第一点，要注意**第一列**的内容，它是代表着软中断的类型，在我的系统里，软中断包括了 10 个类型，分别对应不同的工作类型，比如 `NET_RX` 表示网络接收中断，`NET_TX` 表示网络发送中断、`TIMER` 表示定时中断、`RCU` 表示 RCU 锁中断、`SCHED` 表示内核调度中断。

**·中断次数：**

第二点，要注意同一种类型的软中断在不同 CPU 的分布情况，正常情况下，同一种中断在不同 CPU 上的累计次数相差不多，比如我的系统里，`NET_RX` 在 CPU0 、CPU1、CPU2、CPU3 上的中断次数基本是同一个数量级，相差不多。

**·中断变化速率：**

第三点，这些数值是系统运行以来的累计中断次数，数值的大小没什么参考意义，但是系统的**中断次数的变化速率**才是我们要关注的，我们可以使用 `watch -d cat /proc/softirqs` 命令查看中断次数的变化速率。

**·查看软中断内核线程：**

软中断是以内核线程的方式执行的，我们可以用 `ps` 命令可以查看到，下面这个就是在服务器上查到软中断内核线程的结果：

![image-20230415025302894](D:\typora笔记\UE4笔记\笔记图片\image-20230415025302894.png)	

·内核线程的名字外面都有有中括号，这说明 ps 无法获取它们的命令行参数，所以一般来说，名字在中括号里的都可以认为是内核线程。

·可以看到有 4 个 `ksoftirqd` 内核线程，这是因为我这台服务器的 CPU 是 4 核心的，每个 CPU 核心都对应着一个内核线程。

### ·定位CPU使用率过高问题：

·可以使用 `top` 命令查看，下面是一台服务器上的 top 的数据：

![image-20230415025924896](D:\typora笔记\UE4笔记\笔记图片\image-20230415025924896.png)	

·上图中的黄色部分 `si`，就是 CPU 在软中断上的使用率，而且可以发现，每个 CPU 使用率都不高，两个 CPU 的使用率虽然只有 3% 和 4% 左右，但是都是用在软中断上了。

·其中，CPU 使用率最高的进程也是软中断 `ksoftirqd`，因此可以认为此时系统的开销主要来源于软中断。

·**查看软中断次数最多的软中断类型：**

~~~
使用 watch -d cat /proc/softirqs 命令查看每个软中断类型的中断次数的变化速率。
~~~

![image-20230415030419796](D:\typora笔记\UE4笔记\笔记图片\image-20230415030419796.png)	

一般对于网络 I/O 比较高的 Web 服务器，`NET_RX` 网络接收中断的变化速率相比其他中断类型快很多。

如果发现 `NET_RX` 网络接收中断次数的变化速率过快，接下来就可以使用 `sar -n DEV` 查看网卡的网络包接收速率情况，然后分析是哪个网卡有大量的网络包进来。

![image-20230415030825286](D:\typora笔记\UE4笔记\笔记图片\image-20230415030825286.png)	

接着，在通过 `tcpdump` 抓包，分析这些包的来源，如果是非法的地址，可以考虑加防火墙，如果是正常流量，则要考虑硬件升级等。



### ·小结：

~~~java
为了避免由于中断处理程序执行时间过长，而影响正常进程的调度，Linux 将中断处理程序分为上半部和下半部：

上半部，/*对应硬中断*/，由硬件触发中断，用来快速处理中断；
下半部，/*对应软中断*/，由内核触发中断，用来异步处理上半部未完成的工作；
Linux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，可以通过查看 /proc/softirqs 来观察软中断的累计中断次数情况，如果要实时查看中断次数的变化率，可以使用 watch -d cat /proc/softirqs 命令。

每一个 CPU 都有各自的软中断内核线程，我们还可以用 ps 命令来查看内核线程，一般名字在中括号里面到，都认为是内核线程。

如果在 top 命令发现，CPU 在软中断上的使用率比较高，而且 CPU 使用率最高的进程也是软中断 ksoftirqd 的时候，这种一般可以认为系统的开销被软中断占据了。

这时我们就可以分析是哪种软中断类型导致的，一般来说都是因为网络接收软中断导致的，如果是的话，可以用 sar 命令查看是哪个网卡的有大量的网络包接收，再用 tcpdump 抓网络包，做进一步分析该网络包的源头是不是非法地址，如果是就需要考虑防火墙增加规则，如果不是，则考虑硬件升级等。

~~~





## 为什么0.1+0.2不等于0.3：

### ·负数为什么用补码表示：

·复习：

![image-20230416005645017](D:\typora笔记\操作系统笔记图片\image-20230416005645017.png)	

·「整数类型」的数字在计算机的存储方式，就是将十进制的数字转换成二进制即可。

·以 `int` 类型的数字作为例子，int 类型是 `32` 位的，其中**最高位是作为「符号标志位」**，正数的符号位是 `0`，负数的符号位是 `1`，**剩余的 31 位则表示二进制数据**。

·对于 int 类型的数字 1 的二进制数表示如下：

![image-20230416005925143](D:\typora笔记\操作系统笔记图片\image-20230416005925143.png)	

·负数的存储方式：**所谓的补码就是把正数的二进制全部取反再加 1**

![image-20230416010040216](D:\typora笔记\操作系统笔记图片\image-20230416010040216.png)	

### ·假设不用补码存储负数：

![image-20230416010308448](D:\typora笔记\操作系统笔记图片\image-20230416010308448.png)	

·此时计算-2+1的结果：

![image-20230416010408649](D:\typora笔记\操作系统笔记图片\image-20230416010408649.png)	

![image-20230416010422009](D:\typora笔记\操作系统笔记图片\image-20230416010422009.png)	

·由此可知，如果不用补码存储负数，在计算之前就必须先判断是不是负数才可以进行计算，与其如此不如直接存补码。



### ·十进制与二进制转换：

![image-20230416011218160](D:\typora笔记\操作系统笔记图片\image-20230416011218160.png)	

·最后把「整数部分 + 小数部分」结合在一起后，其结果就是 `1000.101`。

·不是所有的小数都可以用二进制表示，例如0.1转为二进制：

![image-20230416011925860](D:\typora笔记\操作系统笔记图片\image-20230416011925860.png)	

·**由于计算机的资源是有限的，所以是没办法用二进制精确的表示 0.1，只能用「近似值」来表示，就是在有限的精度情况下，最大化接近 0.1 的二进制数，于是就会造成精度缺失的情况**。

·对于二进制小数转十进制时，需要注意一点，小数点后面的指数幂是**负数**。

·二进制 `0.1` 转成十进制就是 `2^(-1)`，也就是十进制 `0.5`，二进制 `0.01` 转成十进制就是 `2^-2`，也就是十进制 `0.25`，以此类推。

![image-20230416012208920](D:\typora笔记\操作系统笔记图片\image-20230416012208920.png)	

### ·计算机存储小数：

·计算机采用浮点数存储小数；

·比如 `1000.101` 这个二进制数，可以表示成 `1.000101 x 2^3`，类似于数学上的科学记数法。	

·该方法在小数点左边只有一个数字，而且把这种整数部分没有前导 0 的数字称为**规格化**，比如 `1.0 x 10^(-9)` 是规格化的科学记数法，而 `0.1 x 10^(-9)` 和 `10.0 x 10^(-9)` 就不是了。

·如果二进制要用到科学记数法，同时要规范化，那么不仅要保证基数为 2，还要保证小数点左侧只有 1 位，而且必须为 1，这样计算机就可以通过这两个信息来确定一个二进制小数。

所以通常将 `1000.101` 这种二进制数，规格化表示成 `1.000101 x 2^3`，其中，最为关键的是 000101 和 3 这两个东西，它就可以包含了这个二进制小数的所有信息：

![image-20230416013438438](D:\typora笔记\操作系统笔记图片\image-20230416013438438.png)	

·上述三个部分的意义如下：

这三个重要部分的意义如下：

- *符号位*：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；
- *指数位*：指定了小数点在数据中的位置（即1.0011 x 2^(-2) 中的-2指数），指数可以是负数，也可以是正数，**指数位的长度越长则数值的表达范围就越大**；
- *尾数位*：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且**尾数的长度决定了这个数的精度**，因此如果要表示精度更高的小数，则就要提高尾数位的长度；



### ·浮点数存储结构示例：

用 `32` 位来表示的浮点数，则称为**单精度浮点数**，也就是我们编程语言中的 `float` 变量，而用 `64` 位来表示的浮点数，称为**双精度浮点数**，也就是 `double` 变量，它们的结构如下：

![image-20230416013714044](D:\typora笔记\操作系统笔记图片\image-20230416013714044.png)	

- double 的尾数部分是 52 位，float 的尾数部分是 23 位，由于同时都带有一个**固定隐含位**（这个后面会说），所以 double 有 53 个二进制有效位，float 有 24 个二进制有效位，所以所以它们的精度在十进制中分别是 `log10(2^53)` 约等于 `15.95` 和 `log10(2^24)` 约等于 `7.22` 位，因此 double 的有效数字是 `15~16` 位，float 的有效数字是 `7~8` 位，这些有效位是包含整数部分和小数部分；

~~~
两个变量的有效数字是通过对位数位进行log计算得到的；

·对数复习：对数是一种数学工具，用于表示一个数在某个基数下的幂。（例如2是基数，就可以表示以2为基数的幂）
log10(1000)=3==10^3，用于表示基数10的幂；

log10(2) * 53==log10(2^53)
~~~



### ·二进制小数转换为浮点数存储：

#### ·案例说明：以 `10.625` 作为例子

![image-20230416014742456](D:\typora笔记\操作系统笔记图片\image-20230416014742456.png)	

1.将10.625转为二进制1010.101；

2.转为类似科学计数法，**把小数点，移动到第一个有效数字后面**，即将 1010.101 右移 `3` 位成 `1.010101`，右移 3 位就代表 +3，左移 3 位就是 -3。

3.移动位数+偏移量=指数；

~~~
float 中的「指数位」就跟这里移动的位数有关系，把移动的位数再加上「偏移量」，float 的话偏移量是 127，相加后就是指数位的值了，即指数位这 8 位存的是 10000010（十进制 130），因此你可以认为「指数位」相当于指明了小数点在数据中的位置。
~~~

`1.010101` 这个数的**小数点右侧的数字就是 float 里的「尾数位」**，由于尾数位是 23 位，则后面要补充 0，所以最终尾数位存储的数字是 `01010100000000000000000`。

#### **为什么要加偏移量？**

·指数可以是正数，也可以是负数；相比无符号整数，有符号的负数在计算式会更加麻烦，因此需要转换为正数再进行存储。

·IEEE标准规定float的指数精度为-127~127，因此只要将指数+127，就可以保证没有负数，因为最小的负数只能是-126；

~~~
比如，指数如果是 8，则实际存储的指数是 8 + 127（偏移量）= 135，即把 135 转换为二进制之后再存储，而当我们需要计算实际的十进制数的时候，再把指数减去「偏移量」即可。如果指数是-127或更小的数，那么这个浮点数就是一个非规格化浮点数。（指数全为0，位数不为0）

非规格化浮点数出现在指数部分全为0，而尾数部分不为0的情况下。这种情况下，尾数部分可以取到比规格化浮点数更小的值。非规格化浮点数的作用是用于表示接近0的小数和0本身。
~~~



### ·浮点数转换：

·最左侧的1不用存储，这是因为 IEEE 标准规定，二进制浮点数的小数点左侧只能有 1 位，并且还只能是 1，**既然这一位永远都是 1，那就可以不用存起来了**。

·因此转换时需要考虑到隐含的1：

![image-20230416020245639](D:\typora笔记\操作系统笔记图片\image-20230416020245639.png)	

·以0.75---1.1的二进制转换为例：

![image-20230416020553297](D:\typora笔记\操作系统笔记图片\image-20230416020553297.png)	



### 0.1+0.2=0.3？

~~~
十进制 0.1 在转换成二进制小数的时候，是一串无限循环的二进制数，计算机是无法表达无限循环的二进制数的，毕竟计算机的资源是有限。
~~~

计算机只能用「近似值」来表示该二进制，那么意味着计算机存放的小数可能不是一个真实值。

现在基本都是用 IEEE 754 规范的「单精度浮点类型」或「双精度浮点类型」来存储小数的，根据精度的不同，近似值也会不同。

![image-20230416020715217](D:\typora笔记\操作系统笔记图片\image-20230416020715217.png)	

~~~
8 位指数部分是 01111011，23 位的尾数部分是 10011001100110011001101，可以看到尾数部分是 0011 是一直循环的，只不过尾数是有长度限制的，所以只会显示一部分，所以是一个近似值，精度十分有限。
~~~

0.2的浮点数：

![image-20230416020836013](D:\typora笔记\操作系统笔记图片\image-20230416020836013.png)	

~~~
可以看到，8 位指数部分是 01111100，稍微和 0.1 的指数不同，23 位的尾数部分是 10011001100110011001101 和 0.1 的尾数部分是相同的，也是一个近似值。

~~~



0.1 的二进制浮点数转换成十进制的结果是 0.100000001490116119384765625：

![image-20230416020940524](D:\typora笔记\操作系统笔记图片\image-20230416020940524.png)

0.2 的二进制浮点数转换成十进制的结果是 `0.20000000298023223876953125`：

![image-20230416021030889](D:\typora笔记\操作系统笔记图片\image-20230416021030889.png)	



·两个结果相加就是 `0.300000004470348358154296875`：

![image-20230416021100749](D:\typora笔记\操作系统笔记图片\image-20230416021100749.png)	

·因此0.1+0.2并不等于0.3；

**因为有的小数无法可以用「完整」的二进制来表示，所以计算机里只能采用近似数的方式来保存，那两个近似数相加，得到的必然也是一个近似数**。

![image-20230416021156831](D:\typora笔记\操作系统笔记图片\image-20230416021156831.png)	

### 小结：

最后，再来回答开头的问题。

> 为什么负数要用补码表示？

负数之所以用补码的方式来表示，主要是为了统一和正数的加减法操作一样，毕竟数字的加减法是很常用的一个操作，就不要搞特殊化，尽量以统一的方式来运算。

> 十进制小数怎么转成二进制？

十进制整数转二进制使用的是「除 2 取余法」，十进制小数使用的是「乘 2 取整法」。

> 计算机是怎么存小数的？

计算机是以浮点数的形式存储小数的，大多数计算机都是 IEEE 754 标准定义的浮点数格式，包含三个部分：

- 符号位：表示数字是正数还是负数，为 0 表示正数，为 1 表示负数；
- 指数位：指定了小数点在数据中的位置，指数可以是负数，也可以是正数，指数位的长度越长则数值的表达范围就越大；
- 尾数位：小数点右侧的数字，也就是小数部分，比如二进制 1.0011 x 2^(-2)，尾数部分就是 0011，而且尾数的长度决定了这个数的精度，因此如果要表示精度更高的小数，则就要提高尾数位的长度；

用 32 位来表示的浮点数，则称为单精度浮点数，也就是我们编程语言中的 float 变量，而用 64 位来表示的浮点数，称为双精度浮点数，也就是 double 变量。

> 0.1 + 0.2 == 0.3 吗？

不是的，0.1 和 0.2 这两个数字用二进制表达会是一个一直循环的二进制数，比如 0.1 的二进制表示为 0.0 0011 0011 0011… （0011 无限循环)，对于计算机而言，0.1 无法精确表达，这是浮点数计算造成精度损失的根源。

因此，IEEE 754 标准定义的浮点数只能根据精度舍入，然后用「近似值」来表示该二进制，那么意味着计算机存放的小数可能不是一个真实值。

0.1 + 0.2 并不等于完整的 0.3，这主要是因为这两个小数无法用「完整」的二进制来表示，只能根据精度舍入，所以计算机里只能采用近似数的方式来保存，那两个近似数相加，得到的必然也是一个近似数。





# ·Linux内核 VS Windows内核：

### ·内核：

·计算机是由各种外部硬件设备组成的，比如内存、cpu、硬盘等，如果每个应用都要和这些硬件设备对接通信协议，那这样太累了，所以这个中间人就由内核来负责，**让内核作为应用连接硬件设备的桥梁**，应用程序只需关心与内核交互，不用关心硬件的细节。

![image-20230416022102849](D:\typora笔记\操作系统笔记图片\image-20230416022102849.png)	

### ·内核的能力：

- 管理进程、线程，决定哪个进程、线程使用 CPU，也就是进程调度的能力；
- 管理内存，决定内存的分配和回收，也就是内存管理的能力；
- 管理硬件设备，为进程与硬件设备之间提供通信能力，也就是硬件通信能力；
- 提供系统调用，如果应用程序要运行更高权限运行的服务，那么就需要有系统调用，它是用户程序与操作系统之间的接口。



### ·内核工作方式：

**内核具有很高的权限**，可以控制 cpu、内存、硬盘等硬件，而**应用程序具有的权限很小**，因此大多数操作系统，把**内存**分成了两个区域：

- 内核空间，这个内存空间只有内核程序可以访问；
- 用户空间，这个内存空间专门给应用程序使用；



### ·用户态-内核态：

·用户空间的代码只能访问一个局部的内存空间，而内核空间的代码可以访问所有内存空间。因此，当程序使用用户空间时，我们常说该程序在**用户态**执行，而当程序使内核空间时，程序则在**内核态**执行。

![image-20230416022415358](D:\typora笔记\操作系统笔记图片\image-20230416022415358.png)	

内核程序执行在内核态，用户程序执行在用户态。当**应用程序使用系统调用**时，会产生一个中断。发生中断后， CPU 会中断当前在执行的用户程序，转而跳转到中断处理程序，也就是开始执行内核程序。**内核处理完后，主动触发中断**，把 CPU 执行权限交回给用户程序，回到用户态继续工作。



### ·Linux系统的设计（简要介绍）

·感觉暂时没啥用，先不做笔记



### ·windowsxit的设计（简要介绍）

·感觉暂时没啥用，先不做笔记



# ·第二章：内存管理

## ·虚拟内存：

单片机是没有操作系统的，所以每次写完代码，都需要借助工具把程序烧录进去，这样程序才能跑起来。

**单片机的 CPU 是直接操作内存的「物理地址」**。

![image-20230416220315510](D:\typora笔记\操作系统笔记图片\image-20230416220315510.png)	

·在这种情况下，要想在内存（单片机中的内存）中同时运行两个程序是不可能的。如果第一个程序在 2000 的位置写入一个新的值，将会擦掉第二个程序存放在相同位置上的所有内容，所以同时运行两个程序是根本行不通的，这两个程序会立刻崩溃。



### ·操作系统如何解决这个问题？

·为每一个进程分配一套属于他们自己的虚拟内存地址，每次都操作自己的内存地址，互不干涉。

·前提：进程不能直接访问物理内存地址。

![image-20230416220902001](D:\typora笔记\操作系统笔记图片\image-20230416220902001.png)	

**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

·当程序需要对虚拟地址进行操作时，操作系统将其转换为不同的物理地址。



### ·两种内存概念：

- 我们程序所使用的内存地址叫做**虚拟内存地址**（*Virtual Memory Address*）
- 实际存在硬件里面的空间地址叫**物理内存地址**（*Physical Memory Address*）。

·操作系统引入了虚拟内存。进程持有的虚拟地址---通过CPU的**内存管理单元（MMU）**的**映射**关系---转变为物理内存。

·再通过转变后的物理内存访问内存空间。

![image-20230416221058846](D:\typora笔记\操作系统笔记图片\image-20230416221058846.png)	

> 操作系统是如何管理虚拟地址与物理地址之间的关系？

主要有两种方式，分别是**内存分段和内存分页**，分段是比较早提出的，我们先来看看内存分段。



### 内存分段：

·**程序**是由若干个**逻辑分段**组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（\*Segmentation\*）的形式把这些段分离出来。**

·如图所示：

![image-20230416222532529](D:\typora笔记\操作系统笔记图片\image-20230416222532529.png)	

### 分段机制下，虚拟地址和物理地址的映射方式：

分段机制下的虚拟地址由两部分组成，**段选择因子**和**段内偏移量**。

![image-20230416222620610](D:\typora笔记\操作系统笔记图片\image-20230416222620610.png)	

#### 段选择因子和段内偏移量：

- **段选择子**就保存在段寄存器里面。段选择子里面最重要的是**段号**，用作段表的索引。**段表**里面保存的是这个**段的基地址、段的界限和特权等级**等。
- 虚拟地址中的**段内偏移量**应该位于 0 和段界限之间，如果段内偏移量是合法的，就将**段基地址加上段内偏移量**得到**物理内存地址**。

~~~
段基地址 + 段内偏移量 = 物理内存地址
~~~



### ·程序案例示例：

虚拟地址是通过**段表**与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址。**（如上表所示）**

~~~
如果要访问段 3 中偏移量 500 的虚拟地址，我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500 = 7500。
~~~



### ·分段的不足之处：

分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处：

- 第一个就是**内存碎片**的问题。
- 第二个就是**内存交换的效率低**的问题。

****



### ·内存碎片

#### ·案例说明：

假设有 1G 的物理内存，用户执行了多个程序，其中：

- 游戏占用了 512MB 内存
- 浏览器占用了 128MB 内存
- 音乐占用了 256 MB 内存。

此时关闭浏览器：空闲内存还有 1024 - 512 - 256 = 256MB；

如果这个 256MB 不是连续的，被分成了两段 128 MB 内存，这就会导致没有空间再打开一个 200MB 的程序。**（外部内存碎片）**

![image-20230416223200118](D:\typora笔记\操作系统笔记图片\image-20230416223200118.png)	

**内存碎片主要分为，内部内存碎片和外部内存碎片。**

~~~
内部内存碎片是指在内存分配时，由于分配的内存大小不是恰好等于请求的大小，而导致的内存浪费。外部内存碎片是指在内存分配时，由于已经分配的内存块被释放或移动，而导致的无法利用的内存空间。因此，内部内存碎片和外部内存碎片是两个不同的概念。
~~~

**内存分段**管理可以做到段根据实际需求分配内存，所以有多少需求就分配多大的段，所以**不会出现内部内存碎片**。

由于每个段的长度不固定，所以多个段未必能恰好使用所有的内存空间，会产生了多个不连续的小物理内存，导致新的程序无法被装载，所以**会出现外部内存碎片**的问题。

~~~java
/*外部内存碎片产生原因：*/

·每个段长度不一致，导致出现多个不连续的物理内存。
~~~

### ·解决方案：

解决「外部内存碎片」的问题就是**内存交换**。

可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。

**这个内存交换空间，在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。**

~~~
通过使用硬盘的存储空间，用于解决内存碎片问题。
~~~



### ·分段导致内存交换效率低：

对于**多进程的系统**来说，用分段的方式，**外部内存碎片是很容易产生**的，产生了外部内存碎片，那不得不重新 `Swap` 内存区域，这个过程会产生**性能瓶颈**。

~~~java
/*因为硬盘的访问速度要比内存慢太多了，每一次内存交换，都需要把一大段连续的内存数据写到硬盘上。
~~~

所以，**如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**

#### ·解决方案：

解决内存分段的「外部内存碎片和内存交换效率低」的问题===》内存分页；

------



### 内存分页：

·分段的好处：产生连续的内存空间；坏处：出现「外部内存碎片和内存交换的空间太大」；

·解决思路：1.减少外部内存碎片；2.减少需要与硬盘进行交换的数据量；

**内存分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小**。这样一个连续并且尺寸固定的内存空间，我们叫**页**（*Page*）。在 Linux 下，每一页的大小为 `4KB`。

![image-20230416224138288](D:\typora笔记\操作系统笔记图片\image-20230416224138288.png)	

·页表是存储在内存里的，**内存管理单元** （*MMU*）就做将虚拟内存地址转换成物理地址的工作。

·而当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、**更新进程页表**，最后再返回用户空间，恢复进程的运行。

------

### ·分页解决分段的「外部内存碎片和内存交换效率低」的问题：

·由于分页的工作方式，所使用的内存空间大小都是事先分配好的，不会像分段一样，段与段之间出现小缝隙。并且页与页之间紧密排列，不会产生外部内存碎片。

**缺点：**

·内存分页中，内存使用的最小单位是页，如果程序非常小，不足以使用完整个“页”的内存，则还是要分配一页给这个程序，此时会产生**内部内存碎片**。



### ·换入---换出：

·当内存空间不够，操作系统将进程中暂时未使用的页面释放（临时写入硬盘中），这称为**换出**。*Swap Out*

·当进程需要这部分数据时，再从硬盘读入内存中，这叫**换入**。*Swap In*

~~~java
此时与硬盘交换的数据一次只有几个页，/*内存交换的效率就相对比较高*/。
~~~

![image-20230416225016304](D:\typora笔记\操作系统笔记图片\image-20230416225016304.png)	

·分页的方式，使程序在加载时，不需要把整个程序加载到内存中。只需要在虚拟内存和物理内存建立映射关系后，（此时还没有把虚拟内存页中需要的指令和数据加载到物理内存中），当程序运行时，需要用到对应虚拟页内存中的指令和数据时，再将对应的数据加载到物理内存中。

------

### 分页机制，虚拟内存和物理内存如何映射？

在分页机制下，虚拟地址分为两部分，**页号**和**页内偏移**。页号作为页表的索引，**页表**包含物理页每页所在**物理内存的基地址**，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。

![image-20230416230216882](D:\typora笔记\操作系统笔记图片\image-20230416230216882.png)	

**·可分为三个步骤：**

- 把虚拟内存地址，切分成页号和偏移量；
- 根据页号，从页表里面，查询对应的物理页号；
- 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

**·（单页表）案例说明：**

![image-20230417005130524](D:\typora笔记\操作系统笔记图片\image-20230417005130524.png)	

·这种简单的分页有空间上的缺陷：

·32位系统，虚拟内存空间有4GB，假设现在每一个页按4KB（2^12）分配空间，则共有100 万 （2^20） 个页，而每个**页表项**需要4个字节存储，则4GB的空间需要用**4MB存储页表项**。

·在多进程的情况下，每个进程都有自己的存储空间，即都有自己的页表。如果100个进程，就需要400MB来存储页表项。

------

### ·多级页表：

·多级页表用于解决上面单页表的空间问题；

·通过将100多万个单页表再分页，将页表（一级页表）分为 `1024` 个页表（二级页表），每个表（二级页表）中包含 `1024` 个「页表项」，形成**二级分页**。

~~~
【这里一级页表一个页表项只占了4个字节，覆盖4G空间只需要4kb（4*1024），而这之后的二级分页覆盖的所有的页内存空间，并且在不需要的时候是可以不用创建的。而不分级页表必须使用4MB来保存上百万个页空间】
~~~



![image-20230417010019153](D:\typora笔记\操作系统笔记图片\image-20230417010019153.png)	

·按照这样分页，根据上面单页表的计算案例，此时存储4GB需要4KB（一级页表）+ 4MB（二级页表）的内存，额外多加了4MB的二级页表空间。

·根据计算机组成原理的**局部性原理**，每个进程都有4GB的虚拟内存空间，但实际上并不是每个进程都会吃满4GB，存在部分对应的页表项都是空的，根本没有分配。对于已分配的页表项，如果存在最近一定时间未访问的页表，在物理内存紧张的情况下，操作系统会将页面换出到硬盘，也就是说不会占用物理内存。

~~~java
/*局部性原理*/是指，计算机在执行程序时，会倾向于访问邻近的内存位置，而不是随机的内存位置。这个原理包括时间局部性和空间局部性两个方面。时间局部性是指，如果一个内存位置被访问了一次，那么在不久的将来它很可能会被再次访问。空间局部性是指，如果一个内存位置被访问了一次，那么在不久的将来它附近的内存位置也很可能会被访问。
    
局部性原理是计算机科学中一个很重要的概念，指的是程序在执行过程中倾向于访问一定范围内的数据，而不是整个数据集。这个“范围”可以是时间局部性（Temporal Locality）或空间局部性（Spatial Locality）。

时间局部性指的是程序中某些数据在短时间内被多次访问的现象，例如循环中的变量，这些变量往往在循环中多次被读取和写入，因此具有时间局部性。

空间局部性指的是程序中访问的数据通常会在物理空间上相邻或接近，例如数组、链表等数据结构，它们在内存中通常是连续存储的，因此具有空间局部性。//（连续存储的数据在读取时会一起被读取）

局部性原理的重要性在于，它可以用来优化计算机程序的性能。因为程序通常只需要访问一小部分数据，所以我们可以将这些数据放入高速缓存（Cache）中，以便快速访问，而不必每次都从主存中读取，从而提高程序的执行效率。
    
//时间局部性：
时间局部性是指程序在执行过程中，倾向于多次访问相同的数据，即同一个数据在短时间内被多次访问。这种局部性通常出现在循环或者函数调用中，例如在一个循环中访问某个变量，每次迭代都会对该变量进行读取或写入操作。

时间局部性的存在可以被用来进行程序优化，例如将/*访问频率高的数据缓存在高速缓存（Cache）中*/，以便下一次访问时可以更快地获取。相比于从主存中读取数据，从缓存中获取数据的速度更快，因为缓存通常位于CPU内部，数据访问的时延更短。

除了在缓存中进行数据存储外，时间局部性还可以被用于其他优化策略，例如循环展开、代码重排等。这些优化策略的目的都是尽可能减少程序中访问主存的次数，从而提高程序的执行效率。

总之，时间局部性是计算机程序中的一种重要的局部性原理，可以用于程序的优化，提高程序的性能和效率。
~~~

·此时使用了二级页表，一级页表目前已经完全覆盖了4GB的内存空间，如果一级页表中某个页表项（二级页表）没有被用到，意味着这个二级页表也就不需要被创建，可以在需要时才创建对应的二级页表。

~~~java
假设只有 20% 的一级页表项被用到了，/*那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）= 0.804MB，*/这对比单级页表的 4MB 是不是一个巨大的节约
~~~

------

#### 页表的性质：

~~~
保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址
~~~

·假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以**页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项**（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。

------

·我们把二级分页再推广到多级页表，就会发现页表占用的内存空间更少了，这一切都要归功于对局部性原理的充分应用。

### ·64位系统的页表：

·64位的系统需要更多的一级目录，同理后面的目录级别也要增加；

对于 64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是：

- 全局页目录项 PGD（*Page Global Directory*）；
- 上层页目录项 PUD（*Page Upper Directory*）；
- 中间页目录项 PMD（*Page Middle Directory*）；
- 页表项 PTE（*Page Table Entry*）；

![image-20230417012358914](D:\typora笔记\操作系统笔记图片\image-20230417012358914.png)	

### ·TBL：页表缓存  |  转址旁路缓存  |  快表

![image-20230417013638567](D:\typora笔记\操作系统笔记图片\image-20230417013638567.png)	

在 CPU 芯片里面，封装了内存管理单元（*Memory Management Unit*）芯片，它用来完成地址转换和 TLB 的访问与交互。

有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。

TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。



### ·段页式内存管理：

·内存分段和内存分页并不是对立的，它们是可以组合起来在同一个系统中使用的，那么组合起来后，通常称为**段页式内存管理**。

·![image-20230417221907800](D:\typora笔记\操作系统笔记图片\image-20230417221907800.png)	

#### ·段页式内存管理实现的方式：

- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；
- 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；

~~~
如上图划分，地址结构就由段号、段内页号和页内位移三部分组成。
~~~

·用于段页式地址变换的数据结构是**每一个程序一张段表**，**每个段又建立一张页表**，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号，如图所示：

![image-20230417222406744](D:\typora笔记\操作系统笔记图片\image-20230417222406744.png)	

段页式地址变换中要得到物理地址须经过三次内存访问：

- 第一次访问段表，得到页表起始地址；
- 第二次访问页表，得到物理页号；
- 第三次将物理页号与页内位移组合，得到物理地址。

可用软、硬件相结合的方法实现段页式地址变换，这样虽然增加了硬件成本和系统开销，但提高了内存的利用率。



### Linux内存布局：

#### ·逻辑地址：

~~~java
在计算机中，逻辑地址（logical address）是指程序中使用的地址，它是相对于/*进程的起始位置而言的*/（/*注意这里说的时一个进程内存的逻辑地址！！进程内部！！*/），通常是一个虚拟地址，也称为程序地址。

逻辑地址是一个抽象的概念，它不是实际存在于计算机内存中的地址。在程序运行时，逻辑地址需要通过地址转换机制转化为实际的物理地址，然后才能被访问到。这个地址转换的过程由硬件的内存管理单元（MMU）完成，它将逻辑地址转换为物理地址，并将其映射到实际的内存地址上。
~~~

#### ·线性地址：

~~~
在线性地址（linear address）模式下，每个进程可以访问整个4GB的虚拟地址空间。这个4GB的虚拟地址空间通常是连续的，因此也被称为“连续地址空间”。

在使用线性地址模式时，操作系统需要提供地址转换机制，将线性地址转换为物理地址，使得程序能够正确访问内存。这个地址转换的过程通常由硬件的内存管理单元（MMU）来完成，MMU将线性地址映射到实际的物理地址上，并提供内存保护和隔离的功能，确保每个进程只能访问自己被允许访问的内存空间。

与逻辑地址不同，线性地址是一个实际存在于计算机内存中的地址，它代表了进程访问内存时的真实地址。由于每个进程都可以访问整个4GB的虚拟地址空间，因此线性地址可以直接用于内存寻址，而不需要进行额外的地址转换。
~~~

·Linux中直接将段内存地址映射到了虚拟内存空间（具体看intel的内存设置），而不需要再通过页式内存管理进行映射。

~~~
在Linux中，每个段都被映射到了虚拟地址空间的固定位置，也就是从0地址开始的整个4GB虚拟空间。这种做法被称为“段式内存管理”，相比于“页式内存管理”，它不需要通过页表等数据结构来实现地址的映射，而是直接将段的起始地址映射到了虚拟地址空间中的固定位置。
~~~



### ·Intel的内存管理机制：

· Intel 的处理器从 80286 开始使用的是段式内存管理，之后再段式内存管理的基础上实现了页式内存管理。

· **页式内存管理的作用是在由段式内存管理所映射而成的地址上再加上一层地址映射。**

· 由于此时由段式内存管理映射而成的地址不再是“物理地址”了，Intel 就称之为**“线性地址”**（也称**虚拟地址**）。于是，段式内存管理先将**逻辑**地址**映射**成**线性**地址，然后再**由页式内存管理**将**线性地址映射成物理地址**。

![image-20230417225255174](D:\typora笔记\操作系统笔记图片\image-20230417225255174.png)	

- 程序所使用的地址，通常是没被段式内存管理映射的地址，称为**逻辑地址**；
- 通过段式内存管理映射的地址，称为线性地址，也叫**虚拟地址**；

~~~
逻辑地址是「段式内存管理」转换前的地址，线性地址则是「页式内存管理」转换前的地址。
~~~

**·Linux的选择：**

· Linux选择避开段式映射的过程。

· **Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是一样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。**

~~~
Linux直接将段映射到了虚拟内存空间，不需要再通过页等数据进行映射，使Intel的 【段式映射--段映射到页】不起作用。
~~~



### ·用户空间-内核空间：

![image-20230417225651551](D:\typora笔记\操作系统笔记图片\image-20230417225651551.png)	

- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是用户空间；
- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。

**内核空间与用户空间的区别：**

- 进程在用户态时，只能访问用户空间内存；
- 只有进入内核态后，才可以访问内核空间的内存；

~~~
每个进程都各自有独立的虚拟内存，但是每个虚拟内存中的内核地址，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。
~~~

![image-20230417225802897](D:\typora笔记\操作系统笔记图片\image-20230417225802897.png)	

#### ·用户空间的内存分布情况：

·32位系统示例图：

![image-20230417230034665](D:\typora笔记\操作系统笔记图片\image-20230417230034665.png)	

通过这张图你可以看到，用户空间内存，从**低到高**分别是 6 种不同的内存段：

- 代码段，包括二进制可执行代码；
- 数据段，包括已初始化的静态常量和全局变量；
- BSS 段，包括未初始化的静态变量和全局变量；
- 堆段，包括动态分配的内存，从低地址开始向上增长；
- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（[跟硬件和内核版本有关 (opens new window)](http://lishiwen4.github.io/linux/linux-process-memory-location)）；
- 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 `8 MB`。当然系统也提供了参数，以便我们自定义大小；

代码段下面还有一段内存空间的（灰色部分），这一块区域是「保留区」，之所以要有**保留区**这是因为在大多数的系统里，我们认为比较小数值的地址不是一个合法地址，例如，我们通常在 C 的代码里会将无效的指针赋值为 NULL。因此，这里会出现一段不可访问的内存保留区，防止程序因为出现 bug，导致读或写了一些小内存地址的数据，而使得程序跑飞。

~~~java
/*保留区*/
在Linux系统中，用户空间的代码段下面通常会有一段被称为“保留区”的空间。这个保留区是为了避免将一些非法地址认为是合法地址而保留的一段空间。

在大多数操作系统中，程序访问内存时需要指定合法的地址，也就是指向已经被操作系统分配并标记为可用的内存区域。一般来说，比较小的地址值往往被视为非法地址，因为这些地址可能会被操作系统或者硬件设备用来存储一些特殊的信息，如硬件寄存器、中断向量表等。如果程序试图访问这些非法地址，就会导致系统崩溃或者产生其他不可预测的错误。

因此，为了避免将一些非法地址当作合法地址，操作系统通常会在用户空间代码段下方保留一定的空间作为保留区，以确保小于某个阈值的地址不会被认为是合法地址。这样做可以增加系统的安全性和稳定性，避免一些潜在的问题。
~~~

~~~
在Linux系统中，通常会将用户空间的代码段下方保留一定的空间作为保留区，以确保小于某个阈值的地址不会被认为是合法地址。这个阈值通常是一个操作系统和硬件共同约定好的值，比如在x86架构中，保留区的大小通常是128KB，也就是说，如果一个用户程序试图访问小于128KB的地址，那么这个地址就会被认为是非法地址，从而避免了一些潜在的问题。这个阈值的大小可以根据具体的系统和硬件架构进行调整。
~~~

### ·小结：

为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独立分配一套**虚拟地址空间**，每个程序只关心自己的虚拟地址就可以，实际上大家的虚拟地址都是一样的，但分布到物理地址内存是不一样的。作为程序，也不用关心物理地址的事情。

每个进程都有自己的虚拟空间，而物理内存只有一个，所以当启用了大量的进程，物理内存必然会很紧张，于是操作系统会通过**内存交换**技术，把不常使用的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换入）。

那既然有了虚拟地址空间，那必然要把虚拟地址「映射」到物理地址，这个事情通常由操作系统来维护。

那么对于虚拟地址与物理地址的映射关系，可以有**分段**和**分页**的方式，同时两者结合都是可以的。

内存分段是根据程序的逻辑角度，分成了栈段、堆段、数据段、代码段等，这样可以分离出不同属性的段，同时是一块连续的空间。但是每个段的大小都不是统一的，这就会导致外部内存碎片和内存交换效率低的问题。

于是，就出现了内存分页，把虚拟空间和物理空间分成大小固定的页，如在 Linux 系统中，每一页的大小为 `4KB`。由于分了页后，就不会产生细小的内存碎片，解决了内存分段的外部内存碎片问题。同时在内存交换的时候，写入硬盘也就一个页或几个页，这就大大提高了内存交换的效率。

再来，为了解决简单分页产生的页表过大的问题，就有了**多级页表**，它解决了空间上的问题，但这就会导致 CPU 在寻址的过程中，需要有很多层表参与，加大了时间上的开销。于是根据程序的**局部性原理**，在 CPU 芯片中加入了 **TLB**，负责缓存最近常被访问的页表项，大大提高了地址的转换速度。

**Linux 系统主要采用了分页管理，但是由于 Intel 处理器的发展史，Linux 系统无法避免分段管理**。于是 Linux 就把所有段的基地址设为 `0`，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相当于屏蔽了 CPU 逻辑地址的概念，所以段只被用于访问控制和内存保护。

另外，Linux 系统中虚拟空间分布可分为**用户态**和**内核态**两部分，其中用户态的分布：代码段、全局变量、BSS、函数栈、堆内存、映射区。

> 最后，说下虚拟内存有什么作用？

- 第一，虚拟内存可以使得进程对运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。
- 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。
- 第三，页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。

### ·提问：

~~~
操作系统中每个进程都有自己的虚拟页表，这些虚拟页表对应的物理地址不会冲突吗？
~~~

~~~
在操作系统中，每个进程都有自己的虚拟地址空间，而这个虚拟地址空间是通过虚拟页表来映射到物理地址空间的。虚拟页表记录了虚拟地址与物理地址之间的映射关系，因此对于不同的进程来说，它们的虚拟页表对应的物理地址是不会冲突的。

虚拟页表是进程私有的数据结构，通常由操作系统维护。在x86架构下，每个进程的虚拟页表都保存在CR3寄存器中指向的物理内存中。因此，当进程切换时，操作系统会根据进程的虚拟页表切换到相应的物理页表，从而实现进程间的隔离和保护。

需要注意的是，在某些情况下，不同进程的虚拟地址空间中可能会存在相同的虚拟地址，但是它们对应的物理地址是不同的。这是因为操作系统在实现虚拟地址到物理地址的映射时，会对不同进程的虚拟地址进行隔离和映射，使得它们的物理地址不会发生冲突。
~~~



## ·malloc如何分配内存：

### ·malloc：

#### ·malloc代码示例：

~~~C
#include <stdio.h>
#include <stdlib.h>

int main() {
    int n = 10;  // 申请的内存块大小
    int* ptr = (int*) malloc(n * sizeof(int));  // 调用 malloc() 函数在堆中分配内存空间，返回一个指向分配内存的指针

    if (ptr == NULL) {  // 如果分配内存失败，输出错误信息
        printf("Error: Unable to allocate memory\n");
        return 1;
    }

    for (int i = 0; i < n; i++) {  // 对分配的内存空间进行初始化，将每个元素赋值为 i
        *(ptr + i) = i;
    }

    for (int i = 0; i < n; i++) {  // 输出分配的内存空间中每个元素的值
        printf("%d ", *(ptr + i));
    }

    free(ptr);  // 释放分配的内存空间

    return 0;
}

~~~

#### ·应用场景：

1. 动态分配内存空间：在程序运行时，需要根据运行时需要动态分配内存空间，以便存储数据。这种情况下，使用 malloc() 函数可以动态分配内存空间，避免浪费内存空间或者内存不足的问题。
2. 实现数据结构：在编写数据结构时，有时需要动态分配内存空间来存储数据，例如链表、堆、树等。使用 malloc() 函数可以方便地动态分配内存空间，以便存储数据结构中的节点或者元素。
3. 文件操作：在进行文件操作时，有时需要读取或者写入大量数据，而这些数据需要在内存中暂存。使用 malloc() 函数可以动态分配足够的内存空间，以便暂存文件中的数据。

#### ·语法：

~~~c
void* malloc(size_t size);
~~~

其中，size 参数指定需要分配的内存空间大小，单位是字节。该函数返回一个指向分配内存空间的 void 类型指针。需要将该指针强制转换为实际需要的类型，以便可以通过指针访问和操作分配的内存空间。

**·注意事项：**

使用 malloc() 函数时需要注意以下几点：

1. 内存分配成功后，需要检查返回的指针是否为 NULL。如果指针为 NULL，则说明内存分配失败。
2. 使用完毕后，需要调用 free() 函数释放分配的内存空间。如果不释放分配的内存空间，将会导致内存泄漏，浪费内存资源。
3. 在分配内存空间时，需要考虑分配的大小是否合适。如果分配的内存空间过小，可能会导致程序崩溃或者数据丢失。如果分配的内存空间过大，可能会浪费内存资源。
4. 在使用分配的内存空间时，需要确保不会越界或者访问未初始化的内存空间。否则，将会导致程序崩溃或者产生不可预知的行为。

**·未初始化的内存空间：**

~~~java
"越界"指的是访问数组或者指针所指向的内存空间范围之外的内存位置。例如，如果一个数组有 10 个元素，但是程序尝试访问第 11 个元素，就会发生越界。

"访问未初始化的内存空间"指的是访问程序中已分配但未初始化的内存空间。例如，使用 malloc() 函数分配了一块内存空间，但是程序没有初始化其中的数据，直接访问这块内存空间中的数据就会出现未定义的行为。这种情况通常会导致程序崩溃、数据丢失等问题。

因此，在使用 malloc() 分配的内存空间时，需要确保访问的范围不会超过分配的大小，并且在使用分配的内存空间之前，需要将其中的数据初始化为合适的值，避免访问未初始化的内存空间。


//已分配但未初始化的内存空间 请再解释一下
在使用 malloc() 函数分配内存空间时，操作系统会在内存中找到一块足够大小的空间，并将该空间的内存地址返回给程序。但是，该内存空间中的数据是不确定的，也就是说，该内存空间中存储的数据是未知的，未经初始化的。

例如，如果使用 malloc() 函数分配一块大小为 10 字节的内存空间，那么该内存空间中的数据是不确定的，可能是任意值。如果程序在使用该内存空间之前不对其进行初始化，就会访问未初始化的内存空间，导致程序出现未定义的行为。

因此，为了避免访问未初始化的内存空间，程序需要在使用分配的内存空间之前，将其中的数据初始化为合适的值。例如，可以使用 memset() 函数将内存空间中的所有字节都设置为 0，或者使用其他的初始化方式。这样，程序就可以安全地使用分配的内存空间，避免出现未定义的行为。
~~~

### ·malloc知识点补充：

1. malloc() 函数定义在 C 标准库头文件 stdlib.h 中。如果要使用 malloc() 函数，需要在程序中包含该头文件。

2. malloc() 函数的原型如下：

   ```
   void *malloc(size_t size);
   ```

   其中，size_t 是一种无符号整型数据类型，用于表示内存空间的大小。函数返回一个 void 型指针，指向分配的内存空间的起始地址。

3. malloc() 函数的参数是需要分配的内存空间的大小，单位是字节（Byte）。例如，如果要分配一个 int 类型数组的内存空间，可以使用 sizeof(int) 乘以数组长度计算需要分配的字节数。

4. malloc() 函数返回的指针需要进行类型转换，才能正确地使用。例如，如果要分配一个 int 类型数组的内存空间，可以使用如下的代码：

   ```
   int *p = (int *)malloc(sizeof(int) * 10);
   ```

   上面的代码将分配一个包含 10 个 int 类型元素的数组的内存空间，并将其起始地址赋值给指针变量 p。

5. 使用完 malloc() 函数分配的内存空间后，需要使用 free() 函数将其释放。例如，如果上面的代码使用完 p 指向的内存空间后，需要释放其占用的内存空间，可以使用如下的代码：

   ```
   free(p);
   ```

   释放内存空间后，p 指向的内存空间将不能再次使用，否则会出现未定义的行为。

6. 如果使用 malloc() 函数分配内存空间失败，函数将返回 NULL 指针。因此，在使用 malloc() 分配内存空间后，需要检查返回值是否为 NULL，以确保内存分配成功。





### ·Linux进程的内存分布：

在 Linux 操作系统中，虚拟地址空间的内部又被分为**内核空间和用户空间**两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统：

​	![image-20230419010641600](D:\typora笔记\操作系统笔记图片\image-20230419010641600.png)

- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是用户空间；
- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。

·每个进程都各自有独立的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。

​	![image-20230419011039666](D:\typora笔记\操作系统笔记图片\image-20230419011039666.png)	

·上面的复习结束，开始看malloc：

malloc() 并不是系统调用，而是 C 库里的函数，用于动态分配内存。

malloc 申请内存的时候，会有两种方式向操作系统申请堆内存。

- 方式一：通过 brk() 系统调用从堆分配内存
- 方式二：通过 mmap() 系统调用在文件映射区域分配内存；

### ·brk请求内存方式：

通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间。如下图：

### 	![image-20230419011309116](D:\typora笔记\操作系统笔记图片\image-20230419011309116.png)	

### ·mmap方式请求内存：

系统调用「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存。如下图：

![image-20230419011507152](D:\typora笔记\操作系统笔记图片\image-20230419011507152.png)	

### ·brk和mmap方式分别在什么时候调用？

malloc() 源码里默认定义了一个阈值：

- 如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；
- 如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；

注意，不同的 glibc 版本定义的阈值也是不同的。	



### ·malloc分配的内存是虚拟内存，不是物理内存；

·分配后的虚拟内存如果没有被访问，虚拟内存不会映射到物理内存，物理内存也就不会被占用。当访问被分配的页表，发现虚拟页表没有与物理页表建立映射关系时，触发**【页中断】**，将物理内存和虚拟内存建立映射关系。



### ·malloc（1）分配的内存大小

·malloc() 在分配内存的时候，会分配比用户申请的内存空间更大的内存作为内存池。

·具体会预分配多大内存，取决于malloc使用的内存管理器。



### ·Ptmalloc2知识点：

Ptmalloc2 是一种可扩展、并发和高性能的内存分配器，是 GNU C 库（glibc）中实现的一种 malloc() 函数。它是对原始的 ptmalloc（phread-based malloc）分配器的改进和升级，是当前 Linux 系统中最常用的一种内存分配器之一。

下面是关于 Ptmalloc2 的一些你需要了解的知识：

1. Ptmalloc2 的实现基于一种称为伙伴系统的动态内存分配算法。这种算法将内存空间划分为不同大小的块，并使用伙伴关系来管理这些块。当申请内存时，分配器会查找一个大小恰好匹配的块，如果没有找到，则将大块划分为两个伙伴，继续查找。
2. Ptmalloc2 支持多线程环境下的内存分配和释放，使用了一些并发控制技术，如互斥锁和条件变量，来保证线程安全性和可扩展性。
3. Ptmalloc2 还使用了一些性能优化技术，如缓存和按需映射等，来提高内存分配的效率和速度。
4. 在实际使用 Ptmalloc2 分配内存时，需要注意一些细节，如内存对齐、内存泄漏和指针管理等。例如，在分配指针类型的变量时，需要确保内存对齐以避免性能问题；在释放内存时，需要确保释放完全，避免内存泄漏问题；在使用指针时，需要注意指针的有效性和正确性，避免指针错误和段错误等问题。
5. 对于一些特殊场景，如大规模内存分配和多线程高并发等，Ptmalloc2 可能不太适用，需要使用其他的内存分配器或自行实现。因此，在使用 Ptmalloc2 分配内存时，需要根据具体场景和需求，选择合适的分配器和优化策略，以获得最佳的性能和效率。



·这里以**malloc默认的Ptmalloc2**进行分析：

~~~c
#include <stdio.h>
#include <malloc.h>

int main() {
  // 打印当前进程的pid，以便后续查看内存分配情况
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());
  
  // 申请1字节的内存，使用malloc函数
  void *addr = malloc(1);
  printf("此1字节的内存起始地址：%x\n", addr);

  // 再次打印进程的pid，以便查看内存分配情况
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());
  /*
  getpid()是一个系统调用函数，用于获取当前进程的进程ID（process ID，简称pid）。它定义在头文件<unistd.h>中，可以在Linux、Unix、macOS等操作系统中使用。getpid()函数没有参数，返回值为pid_t类型的整数，表示当前进程的进程ID。在上面的代码中，getpid()函数用于打印当前进程的pid，以便后续查看内存分配情况。
  */
    
 
  // 等待用户输入，程序阻塞，以便观察当前进程的内存分配情况
  getchar();

  // 释放之前分配的1字节内存，使用free函数
  free(addr);
  printf("释放了1字节的内存，但heap堆并不会释放\n");
  
  // 等待用户输入，程序阻塞，以便观察当前进程的内存分配情况
  getchar();
  return 0;
}

~~~

![image-20230419014336334](D:\typora笔记\操作系统笔记图片\image-20230419014336334.png)	

我们可以通过 /proc/进程号/maps 文件查看进程的内存分布情况。我在 maps 文件通过此 1 字节的内存起始地址过滤出了内存地址的范围。	

```shell
[root@xiaolin ~]# cat /proc/3191/maps | grep d730
00d73000-00d94000 rw-p 00000000 00:00 0                                  [heap]
```

这个例子分配的内存小于 128 KB，所以是通过 brk() 系统调用向堆空间申请的内存，因此可以看到最右边有 [heap] 的标识。

可以看到，堆空间的内存地址范围是 00d73000-00d94000，这个范围大小是 132KB，也就说明了 **malloc(1) 实际上预分配 132K 字节的内存**。

·这里显示的是d73000，而分配的起始地址是d73010，多了0x10，为什么？

~~~java
//TODO
多出来的10字节用于存放内存块信息，用于free释放内存时使用，否则free不知道要释放多少内存
~~~



### free 释放内存，会归还给操作系统吗？

通过 free 释放内存后，堆内存还是存在的，并没有归还给操作系统。

![image-20230419202914975](D:\typora笔记\操作系统笔记图片\image-20230419202914975.png)	

·与其将分配的内存还给系统，不如先存在malloc的内存池，下次如果还需要分配相同的内存，就可以直接调用。

·当进程退出后，会将内存池中的内存返还给系统。

·这种free之后内存还在的情况，**只有在malloc通过brk**（）申请内存才会出现。

·通过**mmap申请内存**，free之后会**马上归还操作系统**；

#### ·代码说明：

~~~c
#include <stdio.h>
#include <malloc.h>

int main() {
  //申请128 KB字节的内存
  void *addr = malloc(128*1024);
  printf("此128KB字节的内存起始地址：%x\n", addr);
  printf("使用cat /proc/%d/maps查看内存分配\n",getpid());

  //将程序阻塞，当输入任意字符时才往下执行
  getchar();

  //释放内存
  free(addr);
  printf("释放了128KB字节的内存，内存也归还给了操作系统\n");

  getchar();
  return 0;
}
~~~

·内存情况：

![image-20230419214531899](D:\typora笔记\操作系统笔记图片\image-20230419214531899.png)	

![image-20230419214554244](D:\typora笔记\操作系统笔记图片\image-20230419214554244.png)	

·释放后会直接归还系统；

#### ·小结：

- malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**；
- malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。



### ·为什么不全用mmap分配内存？

·向系统请求内存，是要通过系统调用的，系统调用就必须从用户态转为内核态，再回到用户态。（花费大量时间）

·如果每次都用mmp分配，则每次都要执行系统调用。

·mmap每次释放内存时，都会归还系统，所以每次mmap分配的虚拟内存都是缺页状态（还没有和物理内存建立映射关系），在第一次访问对应的虚拟内存时，就会触发分页中断。

·**频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大**。

~~~c
//·使用brk（）分配内存的原因：
~~~

**等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了，而且可能这个内存块的虚拟地址与物理地址的映射关系还存在，这样不仅减少了系统调用的次数，也减少了缺页中断的次数，这将大大降低 CPU 的消耗**。



### ·brk这么牛，为什么不全用brk？

#### ·举例说明：

连续申请了 10k，20k，30k 这三片内存，如果 10k 和 20k 这两片释放了，变为了空闲内存空间，如果下次申请的内存小于 30k，那么就可以重用这个空闲内存空间。

![image-20230419215633678](D:\typora笔记\操作系统笔记图片\image-20230419215633678.png)	

·下次申请大于30K的内存时，就没法复用，必须重新向OS申请，此时还有30K的内存空间是保留在内存池里面的，相当于当前已使用的内存是 30K + 大于30K 的内存；

·随着系统频繁地 malloc 和 free ，尤其对于小块内存，堆内将产生越来越多不可用的碎片，导致“内存泄露”。而这种“泄露”现象使用 **valgrind** 是无法检测出来的。

~~~
valgrind
~~~

### free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？

·上面提过 malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节；

·这多出来的16字节就是用于保存内存块的信息：

![image-20230419220000724](D:\typora笔记\操作系统笔记图片\image-20230419220000724.png)	

执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了。



## ·内存满了会发生什么？

### ·虚拟内存的作用：

- 第一，虚拟内存可以使得进程对运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。
- 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。
- 第三，页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。



### ·内存回收的流程：

·通过malloc申请的内存其实是虚拟内存；并不会分配物理内存；

·当应用程序去读写了这块虚拟内存时，CPU就会去访问这个虚拟内存，此时发现该虚拟内存还没有与物理内存建立映射关系；

·此时会触发缺页中断，开始分配物理内存，建立映射关系；

·触发中断时，进程从用户态切换到内核态，将缺页中断交给内核的 **Page Fault Handler** （缺页中断函数）处理。

·缺页中断函数会先检查是否存在空闲的物理内存，如果存在则直接建立映射关系。反之，如果不存在，开始进行**内存回收**流程。

#### ·内存回收的两种方式：

- **后台内存回收**（kswapd）：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程**异步**的，不会阻塞进程的执行。
- **直接内存回收**（direct reclaim）：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是**同步**的，会阻塞进程的执行。

·如果直接内存回收完成后，空闲的物理内存仍然无法满足分配要求，则直接触发OOM（**Out of Memory**）机制；



#### ·**Out of Memory**机制：

·**Out of Memory**会根据算法杀死当前占用物理内存最高的进程，若杀死后还不满足，则继续寻找占用物理内存较高的进程杀死。直到释放足够的物理内存；

![image-20230420212712376](D:\typora笔记\操作系统笔记图片\image-20230420212712376.png)	

### ·哪些内存可以被回收？

·可被分配的内存分为两类：

#### 1.文件页：File-backed Page

·内核缓存的磁盘数据（Buffer）和内核缓存的文件数据（Cache）。这两类文件页。大部分文件页可以直接释放内存，而已经被修改过的文件页（也就是“脏页”）不能直接释放，需要先将修改过的文件页写入磁盘后，才能够被释放。

·**回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存**。

#### 2.匿名页：Anonymous Page

·匿名页没有实际的载体，不像文件缓存那样在磁盘中有实际的磁盘文件载体。匿名页通常是堆，栈等内存数据，这类内存数据通常需要被进程再次访问，不能够直接被释放。因此释放这类内存的方式是直接将不常用的匿名页数据写入磁盘的Swap区（**回收的方式是通过 Linux 的 Swap 机制**），待需要使用时，再从磁盘的swap区读取回来。

~~~
Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。
~~~

~~~c
文件页和匿名页的回收都是基于LRU算法，（优先回收不常访问的内存），这个算法中维护了两个链表，分别是：
//active 和 inactive 两个双向链表
~~~

- **active_list** 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；
- **inactive_list** 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；

·在这两个链表中，所处的位置越接近链表的尾部，表示这块内存页在一段时间内使用的次数越少。这样在内存回收时，就可以先回收位于链表尾部的内存页。



#### ·查询活跃页和不活跃页的大小：

活跃和非活跃的内存页，按照类型的不同，又分别分为文件页和匿名页。可以从 /proc/meminfo 中，查询它们的大小，比如：

~~~c
# grep表示只保留包含active的指标（忽略大小写）
# sort表示按照字母顺序排序
[root@xiaolin ~]# cat /proc/meminfo | grep -i active | sort
Active:           901456 kB
Active(anon):     227252 kB
Active(file):     674204 kB
Inactive:         226232 kB
Inactive(anon):    41948 kB
Inactive(file):   184284 kB
~~~

### ·内存回收的性能影响：

·内存回收分为后台内存回收，这种回收是异步的，不会堵塞线程，所消耗的性能较少；

·但是使用直接内存回收，会直接堵塞进程，导致系统长时间的延长，以及导致CPU利用率升高，最终导致系统负荷飙高。

**·可以被回收的内存类型有两种：文件页和匿名页；**

~~~
1.文件页中的干净页，在回收时能够直接释放，而脏页需要先写入磁盘中，才能被释放，这个过程会发生IO操作。
2.匿名页不能直接回收，需要与Swap区发生读写操作，不常用的写入swap区，需要使用时再读回来，频繁的IO操作会影响系统性能。
~~~

~~~c
//回收内存的操作基本都会发生磁盘 I/O 的，如果回收内存的操作很频繁，意味着磁盘 I/O 次数会很多，这个过程势必会影响系统的性能，整个系统给人的感觉就是很卡。
~~~

#### ·调整匿名页和文件页的回收倾向：

从文件页和匿名页的回收操作来看，文件页的回收操作对系统的影响相比匿名页的回收操作会少一点，因为文件页对于干净页回收是不会发生磁盘 I/O 的，而**匿名页**的 Swap **换入换出这两个操作**都会**发生磁盘 I/O**。

##### ·解决方案：

~~~c
Linux 提供了一个/* /proc/sys/vm/swappiness */选项，用来调整文件页和匿名页的回收倾向。
~~~

swappiness 的范围是 0-100，数值越大，越积极使用 Swap，也就是更倾向于回收匿名页；数值越小，越消极使用 Swap，也就是更倾向于回收文件页。（默认值是60）

~~~shell
[root@xiaolin ~]# cat /proc/sys/vm/swappiness
0
~~~

一般建议 swappiness 设置为 0（默认值是 60），这样在回收内存的时候，会**更倾向于文件页的回收**，但是并不代表不会回收匿名页。



###  尽早触发 kswapd （后台回收）内核线程异步回收内存：

#### ·查看直接内存和后台内存的指标：

~~~
使用 sar -B 1 命令来观察：
~~~

![image-20230420215911758](D:\typora笔记\操作系统笔记图片\image-20230420215911758.png)	

- pgscank/s : kswapd(后台回收线程) 每秒扫描的 page 个数。
- pgscand/s: 应用程序在内存申请过程中每秒直接扫描的 page 个数。
- pgsteal/s: 扫描的 page 中每秒被回收的个数（pgscank+pgscand）。

~~~c
//系统时不时发生抖动，并且在抖动的时间段里如果通过 sar -B 观察到 pgscand 数值很大，那大概率是因为「直接内存回收」导致的。
~~~

#### **·为什么可以得出上面的结论？**

**·1.首先了解系统抖动：**（包括下面的1~4）

~~~c
在 Linux 操作系统中，当内存不足时，操作系统会执行页面回收和交换操作来释放内存。当系统执行这些操作时，如果某些页面需要从磁盘中读取，那么进程可能需要等待页面从磁盘中加载到内存中，这就会导致系统出现延迟和抖动等性能问题。

系统的抖动是指在执行页面回收和交换操作时，/*系统的响应速度变慢或者出现短暂的停顿*/，这通常是由于进程需要等待页面从磁盘中读取或者等待操作系统完成页面回收和交换操作所导致的。当系统出现抖动时，进程的响应速度可能会变慢，从而影响系统的性能和稳定性。
~~~

**·2.内存不足时，Linux做的事：**

~~~c
当系统中的内存不足时，Linux会使用页面回收和交换机制来释放内存。在这种情况下，如果操作系统需要释放某个进程使用的内存页面，但是内存中/*没有足够的空闲页面可以用于分配给其他进程*/，那么系统就需要将/*某些内存页面移动到交换分区或者磁盘上*/，以便为新的页面腾出空间。

因此，当系统执行页面回收操作时，如果操作系统需要将/*某个进程使用的页面移动到磁盘上，那么这个进程需要等待页面从磁盘中读取。这是因为磁盘的访问速度远慢于内存，所以从磁盘中读取数据通常比从内存中读取数据要慢得多*/。当进程需要访问被移动到磁盘上的页面时，它必须等待页面从磁盘中读取并加载到内存中，这会导致系统出现抖动等性能问题，因为进程需要等待页面从磁盘中读取，从而影响系统的响应速度。

为了避免这种情况，通常可以采取一些措施来减轻内存压力，例如增加系统中的内存容量、优化进程的内存使用等。这样可以减少操作系统执行页面回收操作的频率，从而提高系统的性能和响应速度。
~~~

**·3.直接内存回收，如何杀死进程释放内存？**

~~~c
在 Linux 操作系统中，当系统中的内存不足时，操作系统会执行页面回收和交换操作来释放内存。

其中，/*页面回收指的是操作系统将不需要使用的内存页面从进程的地址空间中删除，并将其返回给内存管理子系统以供其他进程使用。*/而页面交换则是将进程当前未使用的内存页面移动到交换分区或磁盘上，并将其从内存中删除。

在执行页面回收操作时，如果操作系统需要回收某个进程使用的内存页面，但是内存中没有足够的空闲页面可以用于分配给其他进程，那么操作系统可能会执行直接内存回收（OOM Killer）操作。OOM Killer 是 Linux 操作系统的一种特殊机制，它可以在系统内存不足时，杀死一些进程来释放内存，以确保系统的正常运行。

在执行直接内存回收操作时，操作系统会选择一些进程并将它们杀死，以释放它们所占用的内存。具体来说，操作系统会首先选择一些占用内存最多、最耗费 CPU 资源或者最久未活动的进程，并尝试向它们发送信号（如 SIGKILL），以通知它们释放内存并终止运行。如果这些进程仍然无法释放足够的内存，则操作系统会选择更多的进程，并重复执行这个过程，直到系统中有足够的内存可用为止。

需要注意的是，在执行直接内存回收操作时，操作系统可能会杀死一些重要的进程，从而导致系统出现异常或崩溃。因此，为了避免直接内存回收对系统造成的影响，我们通常应该采取一些措施来减轻内存压力，例如增加系统中的内存容量、优化进程的内存使用、使用内存压缩等。这样可以减少操作系统执行直接内存回收操作的频率，从而提高系统的性能和稳定性。
~~~

#### **·4.内存压缩知识点：**

~~~c
内存压缩是一种在操作系统中/*利用 CPU 空闲时间对内存页面进行压缩的技术*/，以减少内存使用量并避免页面交换或直接内存回收操作的频繁发生。

具体来说，在执行内存压缩操作时，操作系统会对一些当前未使用的内存页面进行压缩，并将它们存储在内存中。这样一来，可以利用 CPU 空闲时间对内存页面进行压缩，以便将更多的内存空间释放给其他进程使用。

内存压缩技术的实现方式各有不同，常见的实现方式包括压缩算法、预取技术、页面回收机制等。其中，压缩算法可以使用多种压缩算法来对内存页面进行压缩，以减少内存使用量。而预取技术则可以在内存压缩操作执行前，提前将一些内存页面从磁盘中预取到内存中，以加速内存压缩操作的执行速度。此外，操作系统还可以使用页面回收机制来回收一些不需要使用的内存页面，并将它们压缩存储在内存中，以便释放更多的内存空间。

需要注意的是，内存压缩/*虽然可以有效减少内存使用量并避免页面交换或直接内存回收操作的频繁发生，但同时也会占用一定的 CPU 资源，并可能对系统的性能产生一定的影响*/。因此，在使用内存压缩技术时，我们需要根据系统的实际情况和性能需求进行调整和优化。
~~~

·压缩后的内存要使用，需要进行解压操作：**（这两个操作需要消耗CPU资源）**

~~~c
在内存压缩技术中，压缩后的内存需要在使用时解压缩后才能正常使用。当操作系统需要使用被压缩的内存页面时，它会对这些页面进行解压缩操作，将页面从压缩状态还原为原始状态，以便进行读取和写入操作。

在实际应用中，操作系统会对被压缩的内存页面进行管理，以便在需要时能够快速地对其进行解压缩操作。通常情况下，被压缩的内存页面会存储在特殊的压缩缓存区域中，操作系统会在读取或写入压缩页面时，从压缩缓存区域中将页面取出，并进行解压缩操作。这样一来，操作系统可以避免频繁地对内存页面进行压缩和解压缩操作，从而提高系统的性能表现。

需要注意的是，在/*使用内存压缩技术时，由于压缩和解压缩操作需要占用一定的 CPU 资源，*/因此在压缩和解压缩操作时，需要避免对系统的性能造成影响，并进行合理的调整和优化。
~~~



#### ·在了解了上面的四点知识点后，再来看为什么可以得出最上面的那个结论？

在Linux中，sar -B 命令用于查看内存页面回收（paging）和交换（swap）的统计信息。其中，pgscand 表示每秒钟被扫描到但不能释放的页数。

当系统中的内存不足时，Linux会使用页面回收和交换机制来释放内存。当系统中的内存压力较大时，Linux会扫描内存中的页面，并试图将不常用的页面从内存中移除。如果扫描到的页面是不活跃的（即没有被访问或修改），则它们可以被释放，以便在需要时分配给其他进程。如果扫描到的页面是活跃的，则它们不能被释放，并且会被重新加入到页面列表中等待下一次扫描。

当sar -B 命令显示 pgscand 数值很大时，通常意味着系统中的内存压力较大，**需要进行大量的页面回收操作**

（这就意味着需要大量扫描页面，而上面说的【直接内存回收】问题，只是说大概率）。

这可能会导致系统出现抖动等性能问题，因为进程需要等待页面从磁盘中读取，从而影响系统的响应速度。一些可能导致 pgscand 数值很大的情况包括：

- 应用程序使用了大量的内存，导致系统中的内存不足。
- 系统中的其他进程使用了大量的内存，导致系统中的内存不足。
- 系统中的页面缓存占用了大量的内存，导致系统中的内存不足。
- 系统中的I/O操作占用了大量的内存，导致系统中的内存不足。

在这些情况下，通过增加内存或者减少系统中的进程和I/O操作等方式来减轻内存压力可能会有所帮助。

------

#### ·了解完上面的知识点，回到正题，如何尽早的触发后台回收？

内核定义了三个内存阈值（watermark，也称为水位），用来衡量当前剩余内存（pages_free）是否充裕或者紧张，分别是：

- 页最小阈值（pages_min）；
- 页低阈值（pages_low）；
- 页高阈值（pages_high）；

**·这三个内存阈值会划分为四种内存使用情况**

![image-20230420225222053](D:\typora笔记\操作系统笔记图片\image-20230420225222053.png)	

·kswapd 会定期扫描内存的使用情况，根据剩余内存（pages_free）的情况来进行内存回收的工作。

- 图中绿色部分：如果剩余内存（pages_free）大于 页高阈值（pages_high），说明剩余内存是充足的；
- 图中蓝色部分：如果剩余内存（pages_free）在页高阈值（pages_high）和页低阈值（pages_low）之间，说明内存有一定压力，但还可以满足应用程序申请内存的请求；
- 图中橙色部分：如果剩余内存（pages_free）在页低阈值（pages_low）和页最小阈值（pages_min）之间，说明内存压力比较大，剩余内存不多了。**这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值（pages_high）为止**。虽然会触发内存回收，但是不会阻塞应用程序，因为两者关系是异步的。
- 图中红色部分：如果剩余内存（pages_free）小于页最小阈值（pages_min），说明用户可用内存都耗尽了，此时就会**触发直接内存回收**，这时应用程序就会被阻塞，因为两者关系是同步的。

**小结：**

~~~
1.剩余内存（pages_free）大于 页高阈值（pages_high），没事够用
2.剩余内存（pages_free）在页高阈值（pages_high）和页低阈值（pages_low）之间，勉强够用，不用回收
3.剩余内存（pages_free）在页低阈值（pages_low）和页最小阈值（pages_min）之间，有点压力了，需要执行后台回收kswapd0 ，直到内存状态回到1.

4.剩余内存（pages_free）小于页最小阈值（pages_min），要G了，赶紧直接内存回收，杀进程；
~~~



#### ·查看页低阈值：

·页低阈值（pages_low）可以通过内核选项 `/proc/sys/vm/min_free_kbytes` **（该参数代表系统所保留空闲内存的最低限）**来间接设置。

·min_free_kbytes 虽然设置的是页最小阈值（pages_min），但是页高阈值（pages_high）和页低阈值（pages_low）都是根据页最小阈值（pages_min）计算生成的，它们之间的计算关系如下：

~~~c
pages_min = min_free_kbytes//系统所保留空闲内存的最低限
pages_low = pages_min*5/4
pages_high = pages_min*3/2
~~~

如果系统时不时发生抖动，并且通过 sar -B 观察到 pgscand 数值很大，那大概率是因为直接内存回收导致的，这时可以**增大 min_free_kbytes 这个配置**选项来**及早地触发后台回收**，然后继续**观察 pgscand 是否会降为 0**。

~~~
min_free_kbytes 不能随意增大，增大后会让系统保留过多的空闲内存，一定程度上降低了应用程序可使用的内存量，这在一定程度上浪费了内存。

极端情况下设置 min_free_kbytes 接近实际物理内存大小时，留给应用程序的内存就会太少而可能会频繁地导致 OOM 的发生。
~~~

调整 min_free_kbytes 之前，需要先思考一下，应用程序更加关注什么，

1. 如果关注延迟那就适当地增大 min_free_kbytes，
2. 如果关注内存的使用量那就适当地调小 min_free_kbytes。



### ·NUMA架构下的内存回收策略：

#### SMP：

·首先了解SMP架构；

·SMP指的是多个CPU处理器共享资源的**电脑硬件架构**，

~~~
每个 CPU 地位平等，它们共享相同的物理资源，包括总线、内存、IO、操作系统等。每个 CPU 访问内存所用时间都是相同的。

因此，这种系统也被称为一致存储访问结构（UMA，Uniform Memory Access）。
~~~

**·缺点：**

~~~
随着 CPU 处理器核数的增多，多个 CPU 都通过一个总线访问内存，这样总线的带宽压力会越来越大，同时每个 CPU 可用带宽会减少，这也就是 SMP 架构的问题。
~~~

![image-20230421011207615](D:\typora笔记\操作系统笔记图片\image-20230421011207615.png)	

·NUMA 结构，即非一致存储访问结构（Non-uniform memory access，NUMA）。

·NUMA 架构将每个 CPU 进行了分组，每一组 CPU 用 Node 来表示，一个 Node 可能包含多个 CPU 。

**每个 Node 有自己独立的资源，包括内存、IO 等**；

~~~c
每个 Node 之间可以通过互联模块总线（QPI）进行通信，所以，也就意味着/*每个 Node 上的 CPU 都可以访问到整个系统中的所有内存*/。

//访问远端 Node 的内存比访问本地内存要耗时很多。
~~~

#### ·NUMA中的内存回收：

在 NUMA 架构下，当某个 Node 内存不足时，系统可以从其他 Node 寻找空闲内存，也可以从本地内存中回收内存。

具体选哪种模式，可以通过 **/proc/sys/vm/zone_reclaim_mode** 来控制。它支持以下几个选项：

- 0 （默认值）：在回收本地内存之前，在其他 Node 寻找空闲内存；
- 1：只回收本地内存；
- 2：只回收本地内存，在本地回收内存时，可以将文件页中的脏页写回硬盘，以回收内存。
- 4：只回收本地内存，在本地回收内存时，可以用 swap 方式回收内存。

在使用NUMA架构的服务器时，如果系统还有一半内存，却频繁触发【直接内存回收】，大概率zone_reclaim_mode没有设置0，导致内存不足时老是先回收自己的内存，不去其他node中寻找内存；

~~~
访问远端 Node 的内存比访问本地内存要耗时很多，但是相比内存回收的危害而言，访问远端 Node 的内存带来的性能影响还是比较小的。因此，zone_reclaim_mode 一般建议设置为 0。
~~~



### ·保护一个进程不被OOM杀死：

在系统空闲内存不足的情况，进程申请了一个很大的内存，如果直接内存回收都无法回收出足够大的空闲内存，那么就会触发 OOM 机制，内核就会根据算法选择一个进程杀掉。

#### ·如何选择要杀死的进程：

~~~
Linux 内核里有一个 oom_badness() 函数，它会把系统中可以被杀掉的进程扫描一遍，并对每个进程打分，得分最高的进程就会被首先杀掉。
~~~

**·得分因素：**

- 第一，进程已经使用的物理内存页面数。
- 第二，每个进程的 **OOM 校准值 oom_score_adj**。它是可以通过 `/proc/[pid]/oom_score_adj` 来配置的。我们可以在设置 -1000 到 1000 之间的任意一个数值，调整进程被 OOM Kill 的几率。

#### 函数 oom_badness() 里的最终计算方法：

~~~c
// points 代表打分的结果
// process_pages 代表进程已经使用的物理内存页面数
// oom_score_adj 代表 OOM 校准值
// totalpages 代表系统总的可用页面数
points = process_pages + oom_score_adj*totalpages/1000
~~~

**用「系统总的可用页面数」乘以 「OOM 校准值 oom_score_adj」再除以 1000，最后再加上进程已经使用的物理页面数，计算出来的值越大，那么这个进程被 OOM Kill 的几率也就越大**。

·每个进程的oom_score_adj**默认值是0**，最终得分跟进程自身消耗的内存有关，消耗的内存越大越容易被杀掉。



#### ·避免进程被杀死：

- 如果你不想某个进程被首先杀掉，那你可以调整该进程的 oom_score_adj，从而改变这个进程的得分结果，降低该进程被 OOM 杀死的概率。
- 如果你想某个进程无论如何都不能被杀掉，那你可以将 oom_score_adj 配置为 -1000。



### ·小结：

内核在给应用程序分配物理内存的时候，如果空闲物理内存不够，那么就会进行内存回收的工作，主要有两种方式：

- 后台内存回收：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程异步的，不会阻塞进程的执行。
- 直接内存回收：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是同步的，会阻塞进程的执行。

可被回收的内存类型有文件页和匿名页：

- 文件页的回收：对于干净页是直接释放内存，这个操作不会影响性能，而对于脏页会先写回到磁盘再释放内存，这个操作会发生磁盘 I/O 的，这个操作是会影响系统性能的。
- 匿名页的回收：如果开启了 Swap 机制，那么 Swap 机制会将不常访问的匿名页换出到磁盘中，下次访问时，再从磁盘换入到内存中，这个操作是会影响系统性能的。

文件页和匿名页的回收都是基于 LRU 算法，也就是优先回收不常访问的内存。回收内存的操作基本都会发生磁盘 I/O 的，如果回收内存的操作很频繁，意味着磁盘 I/O 次数会很多，这个过程势必会影响系统的性能。

针对回收内存导致的性能影响，常见的解决方式。

- 设置 /proc/sys/vm/swappiness，调整文件页和匿名页的回收倾向，尽量倾向于回收文件页；
- 设置 /proc/sys/vm/min_free_kbytes，调整 kswapd 内核线程异步回收内存的时机；
- 设置 /proc/sys/vm/zone_reclaim_mode，调整 NUMA 架构下内存回收策略，建议设置为 0，这样在回收本地内存之前，会在其他 Node 寻找空闲内存，从而避免在系统还有很多空闲内存的情况下，因本地 Node 的本地内存不足，发生频繁直接内存回收导致性能下降的问题；

在经历完直接内存回收后，空闲的物理内存大小依然不够，那么就会触发 OOM 机制，OOM killer 就会根据每个进程的内存占用情况和 oom_score_adj 的值进行打分，得分最高的进程就会被首先杀掉。

我们可以通过调整进程的 /proc/[pid]/oom_score_adj 值，来降低被 OOM killer 杀掉的概率。

~~~C
//TODO
补充知识点的总结；
~~~

------



## ·在4GB物理内存的机器上，申请8GB内存会发生什么？

·回答这个问题之前，必须要有几个前置条件，否则无法得出答案（或者乱答）

- 操作系统是 32 位的，还是 64 位的？
- 申请完 8G 内存后会不会被使用？
- 操作系统有没有使用 Swap 机制？

**·这个问题需要分场景讨论。**



### ·操作系统虚拟内存大小（复习）

·知识点复习：

·在malloc分配内存时，先分配虚拟内存，等进程访问这块虚拟内存时，CPU寻找对应的物理内存映射，发现找不到，触发缺页中断，由Page Fault Handler （缺页中断函数）处理；

·该函数先检查是否由足够的内存分配，内存不够则开始内存回收，回收后还是不够，开大招OOM。

·内存空间够了就建立映射关系，分配内存。

- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是**用户空间**；
- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。



### ·32位系统的场景：

因为 32 位操作系统，进程最多只能申请 3 GB 大小的虚拟内存空间（对应上面知识点），所以进程申请 8GB 内存的话，在申请虚拟内存阶段就会失败（我手上没有 32 位操作系统测试，我估计失败的错误是 cannot allocate memory，也就是无法申请内存失败）。

### ·64位系统场景：

64 位操作系统，进程可以使用 128 TB 大小的虚拟内存空间，所以进程申请 8GB 内存是没问题的，因为进程申请内存是申请虚拟内存，只要不读写这个虚拟内存，操作系统就不会分配物理内存。

#### ·案例测试：

64位系统，物理内存2G：

![image-20230422223245727](D:\typora笔记\操作系统笔记图片\image-20230422223245727.png)	

·现在，我在机器上，连续申请 4 次 1 GB 内存，也就是一共申请了 4 GB 内存，注意下面代码只是**单纯分配**了虚拟内存，并**没有使用**该虚拟内存：

~~~c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <errno.h>

#define MEM_SIZE 1024 * 1024 * 1024

int main() {
    char* addr[4];
    int i = 0;
    for(i = 0; i < 4; ++i) {
        addr[i] = (char*) malloc(MEM_SIZE);
        if(!addr[i]) {
            printf("执行 malloc 失败, 错误：%s\n",strerror(errno));
		        return -1;
        }
        printf("主线程调用malloc后，申请1gb大小得内存，此内存起始地址：0X%p\n", addr[i]);
    }
    
    //输入任意字符后，才结束
    getchar();
    return 0;
}
~~~

·此代码运行后，因为没有建立映射关系，所以能够正常分配；

![image-20230422223352972](D:\typora笔记\操作系统笔记图片\image-20230422223352972.png)	

·通过下面这条命令查看进程（test）的虚拟内存大小：

~~~shell
# ps aux | grep test
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root      7797  0.0  0.0 4198540  352 pts/1    S+   16:58   0:00 ./test

# ps aux
ps -aux 是以BSD方式显示 
#a 显示所有用户的进程(show processes for all users) u 显示用户(display the process's user/owner) x 显示无控制终端的进程#(also show processes not attached to a terminal)
~~~

~~~c
//ps -aux 是以BSD方式显示  该指令显示的参数：
1、USER 哪个用户启动了这个命令 
2、PID 进程ID 
3、CPU CPU占用率 
4、MEM 内存使用量 
5、VSZ 如果一个程序完全驻留在内存的话需要占用多少内存空间 
6、RSS 当前实际占用了多少内存 
7、TTY: 终端的次要装置号码 (minor device number of tty) 
8、STAT 进程当前的状态("S":中断 sleeping,进程处在睡眠状态,表明这些进程在等待某些事件发生--可能是用户输入或者系统资源的可用性;"D":不可中断 uninterruptible sleep;"R":运行 runnable;"T":停止 traced or stopped;"Z":僵死 a defunct zombie process) 9、START 启动命令的时间点 
10、TIME 进程执行起到现在总的CPU暂用时间 
11、COMMAND 启动这个进程的命令
~~~

VSZ 就代表进程使用的虚拟内存大小，RSS 代表进程使用的物理内存大小。可以看到，VSZ 大小为 4198540，也就是 4GB 的虚拟内存。

#### overcommit_memory：

·能否成功分配虚拟内存，取决于Linux系统中的一个配置， [overcommit_memory]。

~~~c
//可以使用 cat /proc/sys/vm/overcommit_memory 来查看这个参数，这个参数接受三个值：
~~~

- 如果值为 0（默认值），代表：Heuristic overcommit handling，它允许overcommit，但过于明目张胆的overcommit会被拒绝，比如malloc一次性申请的内存大小就超过了系统总内存。Heuristic的意思是“试探式的”，内核利用某种算法猜测你的内存申请是否合理，大概可以理解为单次申请不能超过free memory + free swap + pagecache的大小 + SLAB中可回收的部分 ，超过了就会拒绝overcommit。
- 如果值为 1，代表：Always overcommit. 允许overcommit，对内存申请来者不拒。
- 如果值为 2，代表：Don’t overcommit. 禁止overcommit。

~~~c
//如果内存分配失败，可以调整这个参数 echo 1 > /proc/sys/vm/overcommit_memory
调整为1就可以分配了
这个不是绝对的，不同版本的linux系统可能算法不一样；
~~~



#### ·设置了overcommit_memory后，64位的系统就可以申请128T的内存了吗？

·这个实际上得看实际的物理内存大小，因为在分配虚拟内存的同时，也是有占用物理内存的。（内核保存虚拟内存的数据结构，也是占用物理内存的）

**·案例说明：**

![image-20230423011441281](D:\typora笔记\操作系统笔记图片\image-20230423011441281.png)	

·这里还没有申请到128T的虚拟内存就被杀死了。并且触发了OOM；

#### ·通过top命令查看物理内存使用情况：

~~~
使用 top 命令，点击两下 m，通过进度条观察物理内存使用情况
~~~

![image-20230423011655581](D:\typora笔记\操作系统笔记图片\image-20230423011655581.png)

·直到直接内存回收之后，也无法回收出一块空间供这个进程使用，这个时候就会触发 OOM，给所有能杀死的进程打分，分数越高的进程越容易被杀死。

·在这里当然是这个一直在请求虚拟内存的进程得分最高，那么操作系统就会将这个进程杀死，所以最后会出现 killed，而不是Cannot allocate memory。

------





### ·2G的物理内存不能申请128T的虚拟内存吗？

~~~
答案是 可以。
~~~

·在开启swap后，即可实现；

·开启1GB的swap空间后，效果如下：

![image-20230423012107989](D:\typora笔记\操作系统笔记图片\image-20230423012107989.png)	

![image-20230423012127591](D:\typora笔记\操作系统笔记图片\image-20230423012127591.png)	

·不可能申请128T的，因为系统运行也需要内存。



### ·swap机制的作用：

·当物理内存的容量不足时，就将物理内存中不常用的部分写入硬盘（磁盘）的Swap区，等需要用到时再从磁盘写回内存。

·当内存使用存在压力的时候，会开始触发内存回收行为，会把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。

**·上面的过程都是由Swap机制负责的；**

Swap 就是把一块磁盘空间或者本地文件，当成内存来使用，它包含换出和换入两个过程：

- **换出（Swap Out）** ，是把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存；
- **换入（Swap In）**，是在进程再次访问这些内存的时候，把它们从磁盘读到内存中来；

**·过程图如下：**

![image-20230423014121209](D:\typora笔记\操作系统笔记图片\image-20230423014121209.png)	

#### ·Swap的优缺点：

应用程序实际可以使用的内存空间将远远超过系统的物理内存。

**·缺点：**

频繁地读写硬盘，会显著降低操作系统的运行速率，这也是 Swap 的弊端。



#### ·触发场景：

Linux 中的 Swap 机制会在内存不足和内存闲置的场景下触发：

- **内存不足**：当系统需要的内存超过了可用的物理内存时，内核会将内存中不常使用的内存页交换到磁盘上为当前进程让出内存，保证正在执行的进程的可用性，这个内存回收的过程是**强制的直接内存回收**（Direct Page Reclaim）。直接内存回收是**同步的过程，会阻塞**当前申请内存的进程。
- **内存闲置**：应用程序在启动阶段使用的大量内存在启动后往往都不会使用，通过后台运行的守护进程（kSwapd），我们可以将这部分只使用一次的内存交换到磁盘上为**其他内存的申请预留空间**。**kSwapd** 是 Linux 负责页面置换（Page replacement）的**守护进程**，它也是负责交换闲置内存的**主要进程**，它会在[空闲内存低于一定水位 (opens new window)](https://xiaolincoding.com/os/3_memory/mem_reclaim.html#尽早触发-kSwapd-内核线程异步回收内存)时，回收  内存页中的空闲内存  保证系统中的其他进程可以尽快获得申请的内存。kSwapd 是后台进程，所以回收内存的过程是**异步的，不会阻塞**当前申请内存的进程。

#### ·swap的两种方式：

Linux 提供了两种不同的方法启用 Swap，分别是 Swap 分区（Swap Partition）和 Swap 文件（Swapfile）；

- Swap 分区是硬盘上的独立区域，该区域只会用于交换分区，其他的文件不能存储在该区域上，我们可以使用 `swapon -s` 命令查看当前系统上的交换分区；
- Swap 文件是文件系统中的特殊文件，它与文件系统中的其他文件也没有太多的区别

~~~c
Swapfile会在内存不足时，将内存内容保存到指定的文件上。这样释放出来的内存空间，就可以给到系统其他进程使用了。
    
https://richardweiyang-2.gitbook.io/kernel-exploring/00-index-1/01-swapfile
~~~



#### Swap 换入换出的是什么类型的内存？

1.内核缓存的文件数据，因为都有对应的磁盘文件，所以在回收文件数据的时候， 直接写回到对应的文件就可以了。

2.内存中的堆，栈等数据。这类被称为匿名页，在磁盘中没有实际的载体，并且可能会被再次访问，不能直接释放。因此就需要有一个Swap分区专门用于写入这些数据。



#### ·使用MacOS测试的案例：

~~~
https://xiaolincoding.com/os/3_memory/alloc_mem.html#%E5%AE%9E%E9%AA%8C%E4%B8%80-%E6%B2%A1%E6%9C%89%E5%BC%80%E5%90%AF-swap-%E6%9C%BA%E5%88%B6

·涉及开启swap和未开启swap分区，申请过量的虚拟内存的对比，开启swap分区即使物理内存是8G，申请使用32G也是没问题的。
·但这不意味着可以使用的内存是上限的，当申请的内存过大时，有可能会触发OOM。

·MacOS会根据Swap分区的使用不断加大Swap空间的大小；
Swap 分区总大小是会动态变化的，当没有使用 Swap 分区时，Swap 分区总大小是 0；当使用了 Swap 分区，Swap 分区总大小会增加至 1 GB；当 Swap 分区已使用的大小超过 1 GB 时；Swap 分区总大小就会增加到至 2 GB；当 Swap 分区已使用的大小超过 2 GB 时；Swap 分区总大小就增加至 3GB，如此往复。

·Linux 的分区则是固定大小的，Swap 分区不会根据使用情况而自动增长
~~~





### ·小结：

- 在 32 位操作系统，因为进程理论上最大能申请 3 GB 大小的虚拟内存，所以直接申请 8G 内存，会申请失败。
- 在 64位 位操作系统，因为进程理论上最大能申请 128 TB 大小的虚拟内存，即使物理内存只有 4GB，申请 8G 内存也是没问题，因为申请的内存是虚拟内存。如果这块虚拟内存被访问了，要看系统有没有 Swap 分区：
  - 如果没有 Swap 分区，因为物理空间不够，进程会被操作系统杀掉，原因是 OOM（内存溢出）；
  - 如果有 Swap 分区，即使物理内存只有 4GB，程序也能正常使用 8GB 的内存，进程可以正常运行；

**·设置Swap的实践方案：**

https://support.huaweicloud.com/trouble-ecs/ecs_trouble_0322.html

------





## ·避免预读失效和缓存污染

![image-20230424015529637](D:\typora笔记\操作系统笔记图片\image-20230424015529637.png)	

·这两个问题考的是LRU算法的改进；

**·传统LRU算法存在两个问题：**

- **「预读失效」导致缓存命中率下降（对应第一个题目）**
- **「缓存污染」导致缓存命中率下降（对应第二个题目）**

Redis 的缓存淘汰算法则是通过**实现 LFU 算法**来避免「缓存污染」而导致缓存命中率下降的问题（Redis 没有预读机制）。

MySQL 和 Linux 操作系统是通过**改进 LRU 算法**来避免「预读失效和缓存污染」而导致缓存命中率下降的问题。

### ·本节知识点：

![image-20230424015812281](D:\typora笔记\操作系统笔记图片\image-20230424015812281.png)	

### ·Linux 和 MySQL 的缓存：

### ·Linux 操作系统的缓存：

在应用程序读取文件的数据的时候，Linux 操作系统是会对读取的文件数据进行缓存的，会缓存在文件系统中的 **Page Cache**（如下图中的页缓存）。

![image-20230424020049308](D:\typora笔记\操作系统笔记图片\image-20230424020049308.png)	

·页缓存位于内存中，访问速度更快，如果命中就不需要进行磁盘IO，起到加速数据访问的作用。



### ·MySQL的缓存：

#### ·缓冲池：

·MySQL 的数据是存储在磁盘里的，为了提升数据库的读写性能，Innodb 存储引擎设计了一个**缓冲池**（Buffer Pool），Buffer Pool 属于内存空间里的数据。

![image-20230424020416275](D:\typora笔记\操作系统笔记图片\image-20230424020416275.png)	

### ·传统LRU管理内存的方式：

·因为 Linux 的 Page Cache 和 MySQL 的 Buffer Pool 缓存的**基本数据单位都是页（Page）单位**，所以**后续以「页」名称代替「数据」**。

·Linux的page cache和Mysql的buffer pool，内存空间都不是无限的。当希望一直访问的数据能留在内存中，不常用的数据能够被踢出内存，这就要采用LRU算法。(Least recently used)

~~~c
LRU算法一般是使用链表实现的，常用的数据就放在链表头部，不常用的数据就放在链表的尾部，当空间不足时，就把尾部的数据剔除。
~~~

#### ·传统的LRU思路：

- 当访问的页在内存里，就直接把该页对应的 LRU 链表节点移动到链表的头部。
- 当访问的页不在内存里，除了要把该页放入到 LRU 链表的头部，还要淘汰 LRU 链表末尾的页。

#### ·案例说明：

![image-20230424021204008](D:\typora笔记\操作系统笔记图片\image-20230424021204008.png)	

![image-20230424021223994](D:\typora笔记\操作系统笔记图片\image-20230424021223994.png)	

![image-20230424021243231](D:\typora笔记\操作系统笔记图片\image-20230424021243231.png)

~~~c
传统的 LRU 算法并没有被 Linux 和 MySQL 使用，因为传统的 LRU 算法无法避免下面这两个问题：

//预读失效导致缓存命中率下降；
//缓存污染导致缓存命中率下降；
~~~

​	

### ·预读失效的处理方式：

- 应用程序只想读取磁盘上文件 A 的 offset 为 0-3KB 范围内的数据，由于磁盘的基本读写单位为 block（4KB），于是操作系统至**少会读 0-4KB 的内容**，这恰好可以在一个 page 中装下。
- 但是操作系统出于**空间局部性原理**（靠近当前被访问数据的数据，在未来很大概率会被访问到），会选择将磁盘块 offset [4KB,8KB)、[8KB,12KB) 以及 [12KB,16KB) 都加载到内存，于是**额外在内存中申请了 3 个 page**；

![image-20230424021737165](D:\typora笔记\操作系统笔记图片\image-20230424021737165.png)	

·应用程序利用 read 系统调动读取 4KB 数据，实际上内核使用**预读机制（ReadaHead） 机制完成了 16KB 数据的读取**，也就是通过一次磁盘顺序读将多个 Page 数据装入 Page Cache。这样下次读取4KB之后的数据就可以直接访问内存，不需要读取磁盘。**减少了 磁盘 I/O 次数，提高系统磁盘 I/O 吞吐量**

·MySQL Innodb 存储引擎的 Buffer Pool 也有类似的预读机制，MySQL 从磁盘加载页时，会提前把它相邻的页一并加载进来，目的是为了减少磁盘 IO。

------



### ·预读失效带来的问题：

如果**这些被提前加载进来的页，并没有被访问**，相当于这个预读工作是白做了，这个就是**预读失效**。

如果使用传统的 LRU 算法，就会把「预读页」放到 LRU 链表头部，而当内存空间不够的时候，还需要把末尾的页淘汰掉。

·如果被放在链表头部的预读页一直没有被访问到，而可能是热点数据的页可能会被排到末尾，在内存空间不足时被淘汰。大大降低了缓存命中率。

### ·如何避免缓存失效问题发生？

~~~c
/**让预读页停留在内存里的时间要尽可能的短，让真正被访问的页才移动到 LRU 链表的头部，从而保证真正被读取的热数据留在内存里的时间尽可能长**。
~~~



### ·MySQL和Linux的改良方案：

- Linux 操作系统实现两个了 LRU 链表：**活跃 LRU 链表（active_list）和非活跃 LRU 链表（inactive_list）**；
- MySQL 的 Innodb 存储引擎是在一个 LRU 链表上划分来 2 个区域：**young 区域 和 old 区域**

~~~c
这两个的思路都是为了将冷数据和热数据区别处理，并且分别进行LRU算法。
~~~

#### ·Linux的处理方式：

Linux 操作系统实现两个了 LRU 链表：**活跃 LRU 链表（active_list）和非活跃 LRU 链表（inactive_list）**。

- **active list** 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；
- **inactive list** 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；

**这两个链表分别存储冷数据和热数据，预读页就只需要加入到 inactive list 区域的头部，当页被真正访问的时候，才将页插入 active list 的头部**。如果预读的页一直没有被访问，就会从 inactive list 移除，这样就不会影响 active list 中的热点数据。

------



**·案例说明：**

假设 active list 和 inactive list 的长度为 5，目前内存中已经有如下 10 个页：

![image-20230425022512483](D:\typora笔记\操作系统笔记图片\image-20230425022512483.png)	

·现在有个编号为 20 的页被预读了，这个页只会被插入到 inactive list 的头部，而 inactive list 末尾的页（10号）会被淘汰掉。

![image-20230425022542610](D:\typora笔记\操作系统笔记图片\image-20230425022542610.png)	

**即使编号为 20 的预读页一直不会被访问，它也没有占用到 active list 的位置**，而且还会比 active list 中的页更早被淘汰出去。

~~~
这里就把预读数据作为冷数据处理，不会影响到热数据。
~~~

·如果 20 号页被预读后，立刻被访问了，那么就会将它插入到 active list 的头部， active list 末尾的页（5号），会被**降级**到 inactive list ，作为 inactive list 的头部，这个过程并不会有数据被淘汰。

![image-20230425022708781](D:\typora笔记\操作系统笔记图片\image-20230425022708781.png)	

#### ·MySQL的改善方案：

~~~c
//与Linux的思路类似，都是对冷数据和热数据区分处理
~~~

MySQL 的 Innodb 存储引擎是在一个 LRU 链表上划分来 2 个区域，**young 区域 和 old 区域**。

young 区域在 LRU 链表的前半部分，old 区域则是在后半部分，这两个区域都有各自的头和尾节点，如下图：

![image-20230425022849780](D:\typora笔记\操作系统笔记图片\image-20230425022849780.png)	

young 区域与 old 区域在 LRU 链表中的占比关系并不是一比一的关系，而是 **63:37（默认比例）**的关系。

**划分这两个区域后，预读的页就只需要加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部**。如果预读的页一直没有被访问，就会从 old 区域移除，这样就不会影响 young 区域中的热点数据。	

------

**·案例说明：**

假设有一个长度为 10 的 LRU 链表，其中 young 区域占比 70 %，old 区域占比 30 %。

![image-20230425023036984](D:\typora笔记\操作系统笔记图片\image-20230425023036984.png)	

·现在有个编号为 20 的页被预读了，这个页只会被插入到 old 区域头部，而 old 区域末尾的页（10号）会被淘汰掉。

![image-20230425023107168](D:\typora笔记\操作系统笔记图片\image-20230425023107168.png)	

·如果 20 号页一直不会被访问，它也没有占用到 young 区域的位置，而且还会比 young 区域的数据更早被淘汰出去。

如果 20 号页被预读后，立刻被访问了，那么就会将它插入到 young 区域的头部，young 区域末尾的页（7号），会被挤到 old 区域，作为 old 区域的头部，这个过程并不会有页被淘汰。

![image-20230425023137357](D:\typora笔记\操作系统笔记图片\image-20230425023137357.png)	

------

### ·缓存污染：

·尽管Linux和mysql都对LRU算法进行了改进，预防了缓存失效的影响，但仍然存在缓存污染。

·因为数据只要被使用一次，就会被放到热数据链表的头部。倘若某一个时间段  有很多冷数据被访问，则这些数据都会挤到热数据链表的头部。 如果这些数据有很多**只被访问一次**，之后就不再访问了，那这一类的数据就会把**之前在热数据链表中**的  **真正的热点数据**  **挤出**热数据链表，从而造成**缓存污染**。

~~~c
当我们在批量读取数据的时候，由于数据被访问了一次，这些大量数据都会被加入到「活跃 LRU 链表」里，然后之前缓存在活跃 LRU 链表（或者 young 区域）里的热点数据全部都被淘汰了，如果这些大量的数据在很长一段时间都不会被访问的话，那么整个活跃 LRU 链表（或者 young 区域）就被污染了。
~~~

### 缓存污染会带来什么问题？

缓存污染带来的影响就是很致命的，等  被挤出热数据链表  的那些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 I/O，系统性能就会急剧下降。

#### ·举例说明：

当某一个 SQL 语句**扫描了大量的数据**时，在 Buffer Pool 空间比较有限的情况下，可能会将 **Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了**，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 I/O，MySQL 性能就会急剧下降。

~~~c
//注意， 缓存污染并不只是查询语句查询出了大量的数据才出现的问题，即使查询出来的结果集很小，也会造成缓存污染。
~~~

·例如在数据量很大的表执行了下面的语句：

~~~sql
select * from t_user where name like "%xiaolin%";
~~~

·这句sql会导致索引失效，从而全表查询；

·之后会发生如下的事：

- 从磁盘读到的页加入到 LRU 链表的 old 区域头部；
- 当从页里读取行记录时，也就是**页被访问的时候，就要将该页放到 young 区域头部**；
- 接下来拿行记录的 name 字段和字符串 xiaolin 进行模糊匹配，如果符合条件，就加入到结果集里；
- 如此往复，直到扫描完表中的所有记录。

~~~c
///这样处理后，原本在young区域的数据就都被这句sql读取到的页挤出young区域；
~~~

经过这一番折腾，由于这条 SQL 语句访问的页非常多，每访问一个页，都会将其加入 young 区域头部，那么**原本 young 区域的热点数据都会被替换掉，导致缓存命中率下降**。那些在批量扫描时，而被加入到 young 区域的页，如果在很长一段时间都不会再被访问的话，那么就污染了 young 区域。

![image-20230425024423019](D:\typora笔记\操作系统笔记图片\image-20230425024423019.png)	

可以看到，原本在 young 区域的 6 和 7 号页都被淘汰了，而批量扫描的页基本占满了 young 区域，如果这些页在很长一段时间都不会被访问，那么就对 young 区域造成了污染。

如果 6 和 7 号页是热点数据，那么在被淘汰后，后续有 SQL 再次读取 6 和 7 号页时，由于缓存未命中，就要从磁盘中读取了，降低了 MySQL 的性能，这就是缓存污染带来的影响。

### 怎么避免缓存污染造成的影响？

·改进后的LRU算法，只要访问一次就放到热数据链表头部，这个门槛太低了，需要提高门槛。

·正式因为门槛太低，才导致在发生缓存污染的时候，很容就将原本在活跃 LRU 链表里的热点数据淘汰了。

·由此就可以提出解决方案：

~~~c
//只要我们提高进入到活跃 LRU 链表（或者 young 区域）的门槛，就能有效地保证活跃 LRU 链表（或者 young 区域）里的热点数据不会被轻易替换掉。
~~~

#### ·提高门槛的方式：

- **Linux 操作系统**：在内存页被访问**第二次**的时候，才将页从 inactive list 升级到 active list 里。

- MySQL Innodb：在内存页被访问

  第二次

  的时候，并不会马上将该页从 old 区域升级到 young 区域，因为还要进行

  停留在 old 区域的时间判断：

  - 如果第二次的访问时间与第一次访问的时间**在 1 秒内**（默认值），那么该页就**不会**被从 old 区域升级到 young 区域；
  - 如果第二次的访问时间与第一次访问的时间**超过 1 秒**，那么该页就**会**从 old 区域升级到 young 区域；

~~~
提高了门槛后，就可以很大程度减少缓存污染的影响。
~~~

~~~
在批量读取数据时候，如果这些大量数据只会被访问一次，那么它们就不会进入到活跃 LRU 链表（或者 young 区域），也就不会把热点数据淘汰，只会待在非活跃 LRU 链表（或者 old 区域）中，后续很快也会被淘汰。
~~~

### 小结：

传统的 LRU 算法法无法避免下面这两个问题：

- 预读失效导致缓存命中率下降；
- 缓存污染导致缓存命中率下降；

为了避免「预读失效」造成的影响，Linux 和 MySQL 对传统的 LRU 链表做了改进：

- Linux 操作系统实现两个了 LRU 链表：**活跃 LRU 链表（active list）和非活跃 LRU 链表（inactive list）**。
- MySQL Innodb 存储引擎是在一个 LRU 链表上划分来 2 个区域：**young 区域 和 old 区域**。

但是如果还是使用「只要数据被访问一次，就将数据加入到活跃 LRU 链表头部（或者 young 区域）」这种方式的话，那么**还存在缓存污染的问题**。

为了避免「缓存污染」造成的影响，Linux 操作系统和 MySQL Innodb 存储引擎分别提高了升级为热点数据的门槛：

- Linux 操作系统：在内存页被访问**第二次**的时候，才将页从 inactive list 升级到 active list 里。
- MySQL Innodb：在内存页被访问第二次的时候，并不会马上将该页从 old 区域升级到 young 区域，因为还要进行停留在 old 区域的时间判断：
  - 如果第二次的访问时间与第一次访问的时间**在 1 秒内**（默认值），那么该页就**不会**被从 old 区域升级到 young 区域；
  - 如果第二次的访问时间与第一次访问的时间**超过 1 秒**，那么该页就**会**从 old 区域升级到 young 区域；

通过提高了进入 active list （或者 young 区域）的门槛后，就很好了避免缓存污染带来的影响。

------



## ·Linux虚拟内存管理

### ·目录：

![image-20230426004857103](D:\typora笔记\操作系统笔记图片\image-20230426004857103.png)	

![image-20230426004919677](D:\typora笔记\操作系统笔记图片\image-20230426004919677.png)	

·
